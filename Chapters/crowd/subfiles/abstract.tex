Crowdsourcing systems have been used to accumulate massive amounts of labeled data for applications such as computer vision and natural language processing.
However, because crowdsourced labeling is inherently dynamic and uncertain, developing a technique that can work in most situations is extremely challenging.
In this paper, we introduce Crowd-Certain, a novel approach for label aggregation in crowdsourced and ensemble learning classification tasks that offers superior performance, robustness, and computational efficiency.
The proposed method uses the consistency of the annotators versus a trained classifier to determine a reliability score for each annotator.
Furthremore, Crowd-Certain leverages predicted probabilities, enabling the reuse of trained classifiers on future sample data, thereby eliminating the need for recurrent simulation processes inherent in existing methods.
We extensively evaluated our approach against ten existing techniques across ten different datasets, each labeled by varying numbers of annotators.
The findings demonstrate that Crowd-Certain consistently outperforms the existing methods (Tao, Sheng, KOS, MACE, MajorityVote, MMSR, Wawa, Zero-Based Skill, GLAD, and Dawid Skene), delivering higher average accuracy, F1 score and AUC rates across all tested scenarios, irrespective of the number of annotators involved.
Additionally, we explored the performance of different confidence score measurement techniques, Freq and Beta, using two evaluation metrics: Expected Calibration Error (ECE) and Brier Score Loss.
Our results show that Crowd-Certain consistently achieves higher Brier Score, indicating better-calibrated predictions, and superior performance in terms of ECE across most datasets, suggesting a higher accuracy of probabilistic predictions.

