\chapter{Introduction}

\section{Brief Background on Machine Learning and Uncertainty Management}
The world is currently experiencing an era of rapid development in the fields of machine learning and artificial intelligence. This exciting progression has triggered massive transformations across numerous sectors, including healthcare, automotive industries, and crowdsourcing platforms. In healthcare, machine learning algorithms help in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans. In the automotive industry, the emergence of self-driving vehicles is revolutionizing transportation and mobility. In crowdsourcing, machine learning is used to aggregate information sourced from a large number of individuals to solve complex problems or provide insights. However, as we leverage these technologies to drive innovation and efficiency, we are also confronted with a critical challenge: \textit{management of uncertainty and domain fluctuations in machine learning models}. Real-world applications of machine learning are riddled with situations where the data provided can contain missing, incorrect, or uncertain values. This often results in the unpredictable behavior of the machine learning models, leading to fluctuating performance across different domains.

The precision of model predictions is crucial in high-stakes decision-making situations, such as medical diagnosis or autonomous driving. Given the inherent variability of input data and the susceptibility of models to error, accuracy alone is no longer sufficient. It is crucial to also account for uncertainty in the model's predictions. With sufficient knowledge of model and data uncertainty, we can make informed decisions regarding whether to trust the model's predictions or seek additional information before making a final decision. This capability is particularly important in critical situations where inaccurate predictions could have severe repercussions.

\subsection{Uncertainty in Machine Learning Models}
Uncertainty is an integral part of any predictive model, and machine-learning models are no exception. Recognizing, quantifying, and managing this uncertainty is a critical aspect of developing robust and reliable machine learning systems. Uncertainties in the field of machine learning are typically divided into two categories: aleatoric and epistemic. Aleatoric uncertainty, also referred to as statistical uncertainty, is linked to the data's inherent noise or variability. This type of uncertainty is often irreducible as it stems from factors such as measurement errors or inherent randomness in the system being modeled. Epistemic uncertainty, on the other hand, is related to our lack of knowledge about the system. This form of uncertainty, also known as model uncertainty, arises from our inability to perfectly specify the parameters of our model or from using a model that does not perfectly capture the true underlying process. The sources of uncertainty in machine learning models are complex and can be attributed to various factors. Measurement errors, missing data, and inherent noise are examples of data-related sources of uncertainty. In contrast, model-related sources of uncertainty include the choice of model structure, or model parameters, and the use of approximations in model computations.

Uncertainty can significantly impact the performance and behavior of machine learning models. Unmanaged uncertainty can lead to reduced model performance as the model struggles to make accurate predictions in the face of noisy data or imperfect model specifications. Uncertainty can also lead to biased predictions, as models may overfit to noisy data or become too confident in their predictions, ignoring the inherent uncertainty in the process. Overconfidence is a particularly dangerous effect of unmanaged uncertainty. When a model is overconfident, it may produce very certain predictions even when they are inaccurate. This can lead to poor decision-making, as users of the model may place too much trust in its predictions. In high-stakes domains such as healthcare or autonomous driving, overconfidence can lead to severe negative outcomes. Therefore, understanding and managing uncertainty is a critical task in machine learning. By providing uncertainty estimates along with predictions, machine learning models can become more reliable and trustworthy, facilitating their use in real-world decision-making tasks.

Recent methods tackle the problem of missing and uncertain data, but they assume the data is drawn from well-known distributions defined by a few parameters. Some studies approach the issue of lack of accurate labels by pre-training a network on a large dataset with noisy labels and fine-tuning it for a smaller target dataset~\cite{oquab_Learning_2014}. Semi-supervised techniques have also been proposed where the data with noisy labels is discarded~\cite{zhu_Learning_2002}. However, they suffer from model complexity and cannot be applied to large-scale datasets. Even loss functions that are perceived as noise-robust are not completely robust to label noise~\cite{bartlett_Convexity_2006}. With the rise and influence of ML in medical applications and the need to translate newly developed techniques into clinical practice, questions about the safety and uncertainty of models have gained more importance. Prior research mostly focused on assessing the correctness of individual decisions and modeling the behavior of individual labelers (labeler refers to a human expert or non-expert who assigns labels to data)~\cite{raykar_Supervised_2009}. Label bias becomes more crucial when we move from using manual delineation as our gold standard to using existing software (e.g., Free Surfer for subcortical segmentation tasks).

\section{Dissertation's Objectives and Scope}
The primary objective of this dissertation is to advance the understanding of uncertainty management and, to a lesser extent, domain fluctuation in machine learning models, with the ultimate aim of enhancing models' performance, reliability, and applicability in a variety of real-world settings. We intend to propose, develop, and evaluate novel methods for estimating and mitigating the impact of uncertainty on machine learning outcomes. By doing so, we hope to facilitate the more widespread adoption of these technologies across various sectors and real-world applications and, in the process, harness their full potential.
A significant part of this objective is devoted to investigating ways to incorporate uncertainty information into existing techniques with the goal of improving the accuracy of the prediction models as well as providing an additional confidence score that can enhance the models' interpretability and applicability in real-life settings. We aim to establish a comprehensive understanding of these issues, laying the foundation for the design of new techniques and strategies that manage uncertainty effectively in both supervised and unsupervised settings.

Addressing this challenge, this thesis presents several approaches to estimate and mitigate the effect of uncertainty in model outcomes. We propose strategies to manage uncertainty, aiming to make machine learning models more robust, reliable, and trustworthy. The scope of the thesis is broad, reflecting the wide range of applications and domains in which machine learning models are currently being used. We will focus primarily on healthcare, autonomous driving, and crowdsourcing, as these are areas where either the stakes are high or their applicability is widespread, and thus accurate uncertainty estimates can significantly enhance decision-making and outcomes.

In healthcare, we aim to improve the performance of machine learning models used for tasks such as disease diagnosis and organ segmentation. In the realm of autonomous driving, we introduce a novel transfer learning approach for driver distraction detection. In the field of crowdsourcing and ensemble learning, we have proposed new label aggregation techniques that take into account the consistency and accuracy of annotators (models in the case of ensemble learning). Further, we investigate the impact of domain fluctuation on model accuracy when segmenting the thalamic nuclei and provide a novel technique that facilitates the utilization of low-contrast imaging sequences by developing a model that, albeit trained on advanced imaging technologies,  can readily be used on low-contrast yet more widely available imaging sequences without sacrificing too much accuracy.
By keeping a broad scope, we intend to develop methods that are not only effective in handling uncertainty and domain fluctuations but are also versatile and adaptable across different domains and applications, thereby enhancing the utility of these techniques across different domains.

\section{Dissertation's Organization}
This dissertation contributes to the ongoing research in uncertainty management and domain fluctuation in machine learning models. These approaches take into account the inherent uncertainty and variability in the data, leading to improved model performance. The methods proposed in this thesis provide a framework that can be extended to other machine learning applications. Chapter 2 presents a novel method, ``crowd-certain'', that provides a more accurate and reliable label aggreagtion technique, leading to improved overall performance in both crowdsourcing and ensemble learning scenarios. This method takes into account the consistency and accuracy of the annotators as a measure of their reliability, which allows us to obtain a weight that closely follows the annotator's degree of reliability. Chapter 3 proposes a novel hierarchical multilabel classification technique that utilizes the taxonomic relationship between different classes to improve classification accuracy. Further, to reduce the effect of domain fluctuation and improve the generalizability of the model to images obtained from other sources, the proposed technique provides one model trained on multiple large publicly available chest x-ray datasets (CheXpert~\cite{irvin_CheXpert_2019}, NIH~\cite{wang_ChestXRay8_2017}, and PADCHEST~\cite{bustos_Padchest_2020}). Chapter 4, presents a fast and accurate convolutional neural network-based method for segmentation of thalamic nuclei that is optimized for various diseases, magnetic field strengths, as well as image modalities. It demonstrates the potential of the proposed method for improving our understanding of the thalamic nuclei's involvement in neurological diseases. Finally, Chapters 5 and 6 provide a transfer learning approach for the detection of driver distraction and primary cilia cells, respectively. The proposed technique uses a combination of a convolutional neural network (CNN) and a random decision forest to improve classification accuracy.
