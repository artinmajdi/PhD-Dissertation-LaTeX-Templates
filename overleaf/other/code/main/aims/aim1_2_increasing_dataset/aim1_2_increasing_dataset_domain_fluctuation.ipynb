{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2. Increasing training data using unlabeled datasets\n",
    "\n",
    "Let $X^S$ and $Y^S$ be the source data and its corresponding labels where $y={y_j:   j∈{1,2,…,L}}$ and $y∈Y^S$. A classifier trained using a labeled dataset $D^S≔{(x,y):  x∈X^S,y∈Y^S }$, will be used to classify samples in an unlabeled dataset $D^T≔{(x,∅):  x∈X^T }$  where $X^T$ is the target data and ∅ indicates that a label has not yet been assigned. Each sample’s epistemic uncertainty in the computed labels (e.g., using the method proposed by Yang et al.16) will be measured as follows:\n",
    "\n",
    "\n",
    "$u_j≜ uncertainty (\\hat{y}_{j} | x,W)  ,  ∀x∈X^{T}  ,j∈{1,2,…,L}$\n",
    "\n",
    "\n",
    "where $\\hat{y}_j∈{0,1}$ is the predicted value for class j associated with data x given weights W (weights of a model trained on source dataset D^S). The samples with uncertainty values below a threshold $(γ)$ will be selected as follows:\n",
    "\n",
    "\n",
    " $\\hat{D}^T≜{(x ,\\hat{y}_j ): 1/L ∑_{j=1}^L u_j < γ ,  x ∈ X^{T},  j∈{1,2,…,L}}$\n",
    "\n",
    "\n",
    "Finally, we will add all these selected samples along with their computed labels to the original training dataset, $ D^{H} ≜ D^{S} ∪ \\hat{D}^{T} $.\n",
    "\n",
    "In Bayesian DNNs, we need to compute the full posterior $p(W│D^S )$. Different datasets can have different distributions, which makes measuring the priory $p(D^S)$ troublesome. Thus, to apply the model to new cases, one need to calculate the posterior predictive distribution $p(y_j=1│x,D^S )=∫p(y_j=1│x,W)  p(W|D^S )*dW$ instead. This is hard. Instead, the posterior can be approximated using a variational distribution Q(W) by minimizing the Kullback-Leibler (KL) divergence between Q(W) and posterior. The posterior predictive distribution can then be approximated through Monte-Carlo sampling. \n",
    "\n",
    "$p(y_j=1 │ x,D^S )=∫〖p(y_j=1│x,W)  Q(W)  dW〗≈1/G ∑_{g=1}^G〖p(y_j=1│x,W_g ) 〗  ,∀ x∈X^T,  j∈{1,2,…,L}$, \n",
    "\n",
    "where $G$ is the number of Monte-Carlo samples, $W_g$ are the weights after applying Monte Carlo dropout to original weights $W$.\n",
    "\n",
    "\n",
    "Uncertainty can be measured by measuring the std of these samples:\n",
    "\n",
    "\n",
    "uncertainty $(\\hat{y}_j | x,W) =$ std $(p(y_j=1│x,W_g ),∀g∈G)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import funcs \n",
    "import load_data\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "from time import time\n",
    "import git\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%reload_ext load_data\n",
    "%reload_ext funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "1. what are the state of art techniques for increasing datasets (e.g., data augmentaion, unsupervised techniques, etc.) and see how much our technique improve the accuracy with comparison to those\n",
    "\n",
    "2. we should only use a portion of the original dataset(chest) to train the original model to see how much this technique can improve the results. this can also be a variable to see the relation between the dataset size and change in accuracy\n",
    "\n",
    "3. maybe 1 juornal paper for the main technique, and then a conference paper for the domain fluctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of pathologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathologies = [\"No Finding\", \"Enlarged Cardiomediastinum\" , \"Cardiomegaly\" , \"Lung Opacity\" , \"Lung Lesion\", \"Edema\" , \"Consolidation\" , \"Pneumonia\" , \"Atelectasis\" , \"Pneumothorax\" , \"Pleural Effusion\" , \"Pleural Other\" , \"Fracture\" , \"Support Devices\"]\n",
    "\n",
    "# TODO 1. run it for multiple thresholds to plot a graph\n",
    "# TODO 2. find multi-label datasets (https://www.uco.es/kdis/mllresources/)\n",
    "\n",
    "# Values I get from this results:\n",
    "#       1. Improve the accuracy on existing models by introducing the model to multiple version of the test image\n",
    "#       2. Increase the size of existing datasets by adding the unlabeled data to it\n",
    "#       3. Can help with domain fluctuation. train on one domain, test on another using the ATT method (is this the monte carlo method or its different? more details in the proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssh-tunnel to server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'ssh -N -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org &'\n",
    "ssh_session = subprocess.Popen('exec ' + command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up mlflow config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the server config\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "\n",
    "# setting the server uri\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "\n",
    "# Setting up the experiment\n",
    "experiment_name = 'expanding_dataset_aim1_2' # 'label_inter_dependence'\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# Running the PARENT simulation holding the optimized model\n",
    "run_id_parent = 'bc306d0c76b94e19845f442f143fd5df' \n",
    "\n",
    "# starting the parent session\n",
    "session_parent = mlflow.start_run(run_id=run_id_parent)\n",
    "\n",
    "# starting the child session\n",
    "session_child = mlflow.start_run(nested=True) # run_name='test' or run_id='\n",
    "\n",
    "# mlflow.set_tag('run_id',session_child.info.run_id)\n",
    "# mlflow.start_run(run_id='18aa1eac47264f4dba07fce44984b438')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sample-pruning\n",
      "train: (223414, 19)\n",
      "test: (234, 19)\n",
      "\n",
      "after sample-pruning\n",
      "train (certain): (567, 20)\n",
      "train (uncertain): (291, 20)\n",
      "valid: (142, 20)\n",
      "test: (169, 20) \n",
      "\n",
      "Found 291 validated image filenames.\n",
      "Found 291 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "mode_labels = 'uncertain'\n",
    "Data, Info, data_generator, data_generator_aug = load_data.load_chest_xray_with_mode(dataset='chexpert', mode=mode_labels, max_sample=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the optimized model from server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "# mlflow.set_experiment(experiment_name='soft_weighted_MV_aim1_3')\n",
    "model = mlflow.keras.load_model(model_uri='runs:/{}/model'.format(run_id_parent),compile=False)\n",
    "\n",
    "#  Compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss      = funcs.weighted_bce_loss(Info.class_weights) \n",
    "metrics   = [tf.keras.metrics.binary_accuracy]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracy after changing the nan labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ignore</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>maximum</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pos</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <td>51.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>88.5</td>\n",
       "      <td>neg</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <td>61.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>66.6</td>\n",
       "      <td>neg</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Opacity</th>\n",
       "      <td>93.2</td>\n",
       "      <td>91.8</td>\n",
       "      <td>53.3</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Lesion</th>\n",
       "      <td>12.5</td>\n",
       "      <td>8.2</td>\n",
       "      <td>87.5</td>\n",
       "      <td>neg</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edema</th>\n",
       "      <td>69.2</td>\n",
       "      <td>69.8</td>\n",
       "      <td>40.4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>66.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>93.4</td>\n",
       "      <td>neg</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>76.4</td>\n",
       "      <td>12.1</td>\n",
       "      <td>90.9</td>\n",
       "      <td>neg</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>34.9</td>\n",
       "      <td>12.5</td>\n",
       "      <td>76.7</td>\n",
       "      <td>neg</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>88.2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>49.1</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <td>87.8</td>\n",
       "      <td>85.4</td>\n",
       "      <td>46.7</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Other</th>\n",
       "      <td>22.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>97.5</td>\n",
       "      <td>neg</td>\n",
       "      <td>75.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fracture</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>neg</td>\n",
       "      <td>73.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Devices</th>\n",
       "      <td>81.4</td>\n",
       "      <td>72.5</td>\n",
       "      <td>64.8</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ignore    pos   neg maximum  change\n",
       "No Finding                     0.0  100.0   0.0     pos   100.0\n",
       "Enlarged Cardiomediastinum    51.1   11.8  88.5     neg    37.4\n",
       "Cardiomegaly                  61.1   37.8  66.6     neg     5.5\n",
       "Lung Opacity                  93.2   91.8  53.3  ignore    -1.4\n",
       "Lung Lesion                   12.5    8.2  87.5     neg    75.0\n",
       "Edema                         69.2   69.8  40.4     pos     0.6\n",
       "Consolidation                 66.6   12.2  93.4     neg    26.8\n",
       "Pneumonia                     76.4   12.1  90.9     neg    14.5\n",
       "Atelectasis                   34.9   12.5  76.7     neg    41.8\n",
       "Pneumothorax                  88.2   62.2  49.1  ignore   -26.0\n",
       "Pleural Effusion              87.8   85.4  46.7  ignore    -2.4\n",
       "Pleural Other                 22.2    0.6  97.5     neg    75.3\n",
       "Fracture                      20.0    3.1  93.6     neg    73.6\n",
       "Support Devices               81.4   72.5  64.8  ignore    -8.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measuring the accuracy\n",
    "MA = funcs.Measure_Accuracy_Aim1_2(predict_accuracy_mode=True, generator=data_generator, model=model, how_to_treat_nans='ignore')\n",
    "prob_ignore, acc_ignore = MA.loop_over_whole_dataset()\n",
    "\n",
    "MA = funcs.Measure_Accuracy_Aim1_2(predict_accuracy_mode=True, generator=data_generator, model=model, how_to_treat_nans='pos')\n",
    "prob_pos, acc_pos = MA.loop_over_whole_dataset()\n",
    "\n",
    "MA = funcs.Measure_Accuracy_Aim1_2(predict_accuracy_mode=True, generator=data_generator, model=model, how_to_treat_nans='neg')\n",
    "prob_neg, acc_neg = MA.loop_over_whole_dataset()\n",
    "\n",
    "\n",
    "# converting to dataframe\n",
    "df = pd.DataFrame({'ignore':acc_ignore , 'pos':acc_pos, 'neg':acc_neg},index=pathologies)\n",
    "\n",
    "df['maximum'] = df.columns[ df.values.argmax(axis=1) ]\n",
    "df['change']  = df[['neg','pos']].max(axis=1) - df['ignore']\n",
    "\n",
    "df = df[['ignore','pos','neg','maximum','change']]\n",
    "\n",
    "df.maximum[df.change==0.0] = '--'\n",
    "df.change[df.change==0.0] = '--'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viewing the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_aug.reset()\n",
    "sample_index = 25\n",
    "\n",
    "x = {}\n",
    "for j in range(1,4):\n",
    "\n",
    "    data_generator_aug.batch_index = 0\n",
    "    x, _  = next(data_generator_aug)\n",
    "\n",
    "    ax = plt.subplot(1,3,j)\n",
    "    ax.imshow(x[sample_index,...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the accuracy after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = {}\n",
    "    columns = ['old-accuracy', 'new-accuracy', 'std']\n",
    "    accuracies_all_modes = {mode:{} for mode in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for how_to_treat_nans in ['ignore', 'pos', 'neg']:\n",
    "\n",
    "    print('How to treat nans:',how_to_treat_nans)\n",
    "\n",
    "    all_outputs, MA = funcs.apply_technique_aim_1_2(how_to_treat_nans=how_to_treat_nans, data_generator=data_generator, data_generator_aug=data_generator_aug, model=model, uncertainty_type='std')\n",
    "\n",
    "    df[how_to_treat_nans] = funcs.estimate_maximum_and_change(all_accuracies=all_outputs, pathologies=pathologies)\n",
    "    \n",
    "    for mode in columns: \n",
    "        accuracies_all_modes[mode][how_to_treat_nans] = all_outputs[mode]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA.probs_avg_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_all_modes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neg'].plot.barh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the results to uncertain labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. generating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DenseNet121'\n",
    "\n",
    "# without augmentation and therefore no uncertainty\n",
    "MA = funcs.Measure_Accuracy_Aim1_2(predict_accuracy_mode=True, generator=data_generator, model=model, how_to_treat_nans='ignore', uncertainty_type='std')\n",
    "\n",
    "prob_orig, _ = MA.loop_over_whole_dataset()\n",
    "\n",
    "\n",
    "# with augmentation and uncertainty\n",
    "accuracies, MA = funcs.apply_technique_aim_1_2(how_to_treat_nans='ignore', data_generator=data_generator, data_generator_aug=data_generator_aug, model=model, uncertainty_type='std')\n",
    "\n",
    "probs_avg_2d = MA.probs_avg_2d\n",
    "probs_std_2d = MA.probs_std_2d\n",
    "\n",
    "\n",
    "# saving the effect of augmentation as artifact\n",
    "df_effect = funcs.estimate_maximum_and_change(all_accuracies=accuracies, pathologies=pathologies)\n",
    "\n",
    "\n",
    "dataframes = Data.dataframe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. downloading the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the server config\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "\n",
    "# setting the server uri\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# Setting up the experiment\n",
    "experiment_name = 'expanding_dataset_aim1_2'\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# starting the parent session\n",
    "run_id = 'bc306d0c76b94e19845f442f143fd5df'\n",
    "mlflow.start_run(run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the processed dataframe\n",
    "dir = '/groups/jjrodrig/projects/chest/dataset/chexpert/'\n",
    "train_raw = pd.read_csv(dir + '/train.csv')\n",
    "(df_train, df_train_uncertain), df_valid, df_test, pathologies, class_weights = load_data.chexpert(dir=dir,max_sample=10000000)\n",
    "\n",
    "dataframes = {'train_raw':train_raw, 'train':df_train, 'valid':df_valid, 'uncertain':df_train_uncertain, 'test':df_test}\n",
    "\n",
    "\n",
    "# downloading the probabilities and uncertainties\n",
    "local_dir = '../../increase_dataset/' \n",
    "# os.mkdir(local_dir)\n",
    "full_path = mlflow.tracking.MlflowClient().download_artifacts(run_id, '', local_dir)\n",
    "\n",
    "model = 'DenseNet121'\n",
    "prob_orig    = pd.read_csv(full_path + 'prob_' + model + '_orig.csv'  ,index_col=['Unnamed: 0'])\n",
    "probs_avg_2d = pd.read_csv(full_path + 'prob_' + model + '_aug.csv'   ,index_col=['Unnamed: 0'])\n",
    "probs_std_2d = pd.read_csv(full_path + 'uncertainty_' + model + '.csv',index_col=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan, 0.52242094, 0.43286486, 0.93319385, 0.15750487,\n",
       "       0.52797478, 0.68221024, 0.79607109, 0.41781095, 0.44198895,\n",
       "       0.80347517, 0.40163934, 0.0852159 , 0.62468958])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measuring accuracy\n",
    "prediction = probs_avg_2d.reset_index().copy()\n",
    "truth = dataframes['uncertain'].reset_index().copy()\n",
    "\n",
    "prediction[pathologies] = prediction[pathologies] > 0.5\n",
    "truth = truth[pathologies].replace(-5,np.nan).replace(-10,np.nan).replace(1,True).replace(0,False)\n",
    "\n",
    "func = lambda x1, x2: [ (x1[j] > 0.5) == (x2[j] > 0.5) for j in range(len(x1))]\n",
    "pred_acc = truth[pathologies].combine(prediction[pathologies],func=func) # .to_list()\n",
    "pred_acc = pred_acc.set_index(truth.index)\n",
    "pred_acc[truth.isnull()] = np.nan\n",
    "\n",
    "accuracy = np.nanmean(np.array(pred_acc),axis=0)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(dir + '/train.csv')\n",
    "\n",
    "indexes = probs_avg_2d.index\n",
    "train_uncertain  = train_raw.loc[indexes,pathologies] \n",
    "null_locations = train_uncertain.replace(0,np.nan).isnull().to_numpy()\n",
    "\n",
    "# extracting the certain values\n",
    "uncertainty_threshold = 0.25\n",
    "null_locations[probs_std_2d[pathologies] > uncertainty_threshold] = False\n",
    "\n",
    "train_uncertain = train_uncertain.to_numpy()\n",
    "predictions     = (probs_avg_2d[pathologies] > 0.5).replace(True,1).replace(False,-1)\n",
    "\n",
    "# replacing the uncertain cells with predicted values\n",
    "train_uncertain[null_locations]    = predictions.to_numpy()[null_locations]\n",
    "\n",
    "train_raw.loc[indexes,pathologies] = pd.DataFrame(train_uncertain,columns=pathologies)\n",
    "\n",
    "# replacing all remainign nan cells to -1 or negative label\n",
    "train_raw = train_raw.replace(np.nan,-1)\n",
    "\n",
    "# saving the final results\n",
    "train_raw.to_csv(dir + '/train_aim1_2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the child mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the parent mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the results as artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes[mode_labels].shape\n",
    "# prob_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(path, df_info, outputs, pathologies):\n",
    "    df = df_info.drop(pathologies,axis=1)\n",
    "    df_temp = pd.DataFrame(outputs,columns=pathologies).set_index(df.index)\n",
    "    df[pathologies] = df_temp[pathologies]\n",
    "    df.to_csv(path)\n",
    "\n",
    "model_name = 'DenseNet121'\n",
    "mode_labels = 'test'\n",
    "\n",
    "mlflow.log_param('test','successful')\n",
    "# without augmentation and therefore no uncertainty\n",
    "path = f'../../prob_{model_name}_orig.csv'\n",
    "save_artifacts(path=path, df_info=dataframes[mode_labels], outputs=prob_orig, pathologies=pathologies)\n",
    "mlflow.log_artifact(path,artifact_path='')\n",
    "\n",
    "\n",
    "# saving the augmentation and uncertainty data\n",
    "path = f'../../prob_{model_name}_aug.csv'\n",
    "save_artifacts(path=path, df_info=dataframes[mode_labels], outputs=probs_avg_2d, pathologies=pathologies)\n",
    "mlflow.log_artifact(path,artifact_path='')\n",
    "\n",
    "\n",
    "path = f'../../uncertainty_{model_name}.csv'\n",
    "save_artifacts(path=path, df_info=dataframes[mode_labels], outputs=probs_std_2d, pathologies=pathologies)\n",
    "mlflow.log_artifact(path,artifact_path='')\n",
    "\n",
    "\n",
    "# saving the effect of augmentation as artifact\n",
    "# path = f'../../effect_of_augmentation_{model_name}.csv'\n",
    "# df_effect.to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = prob_aug > 0.5\n",
    "# pred_labels\n",
    "mlflow.log_artifact('/home/u29/mohammadsmajdi/projects/chest_xray/prob_DenseNet121_aug2.csv',artifact_path='')\n",
    "mlflow.end_run()\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO apply the predicted probs to uncertain sampels\n",
    "\n",
    "# # all_outputs['MA'].probs_avg_2d\n",
    "# # all_outputs['MA'].probs_std_2d\n",
    "# truth = data_generator.labels.copy()\n",
    "\n",
    "# # changing the null lables according to above results\n",
    "# (truth==-10) and (all_outputs['MA'].probs_std_2d < 0.3)\n",
    "# # uncertain_indexes = np.where(truth==-10 and all_outputs['MA'].probs_std_2d < 0.3)\n",
    "# # truth[uncertain_indexes] = all_outputs['MA'].probs_avg_2d[uncertain_indexes] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(data_generator.labels)\n",
    "# all_outputs['MA'].probs_avg_2d[uncertain_indexes]\n",
    "# np.where(all_outputs['MA'].probs_std_2d > 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# killing the mlflow & ssh sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the child mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the parent mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa8724247638d5e254d9d19df2232ec995d2e23674e16916912bd569c0dd30b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('paper_miniforge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "97f50b47c5db4a373caba7d351ed0bd803d6a9b66b6e99b50d57389022e4f55d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
