{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from utils import funcs, load_data\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import subprocess\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%reload_ext load_data\n",
    "%reload_ext funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def running_evaluation(dataset='valid', pathologies=['pathologies'], Data='', model='', number_augmentation=3):\n",
    "\n",
    "    def log_results(dataframe, probs_2d_orig, pathologies, MA, dataset):\n",
    "\n",
    "        def add_dataframe_info_columns(df_info, probs_2d, pathologies):\n",
    "\n",
    "            df              = df_info.drop(pathologies,axis=1)\n",
    "            df_temp         = pd.DataFrame(probs_2d_orig, columns=pathologies).set_index(df.index)\n",
    "            df[pathologies] = df_temp[pathologies]\n",
    "\n",
    "            return df\n",
    "            \n",
    "        path = f'../../prob_{dataset}.csv'\n",
    "        df = add_dataframe_info_columns(df_info=dataframe, probs_2d=probs_2d_orig, pathologies=pathologies)       \n",
    "        df.to_csv(path)\n",
    "        mlflow.log_artifact(path,artifact_path=f'probabilities/{dataset}/')\n",
    "\n",
    "        path = f'../../prob_aug_avg_{dataset}.csv'\n",
    "        pd.DataFrame( MA.probs_avg_2d, columns=Info.pathologies ).to_csv(path)\n",
    "        mlflow.log_artifact(path,artifact_path=f'probabilities/{dataset}/')\n",
    "\n",
    "        path = f'../../uncertainty_{dataset}.csv'\n",
    "        pd.DataFrame( MA.probs_std_2d, columns=Info.pathologies ).to_csv(path)\n",
    "        mlflow.log_artifact(path,artifact_path=f'uncertainties/{dataset}/')\n",
    "\n",
    "\n",
    "        path = f'../../accuracy_orig_{dataset}.csv'\n",
    "        accuracy = np.floor( 1000*np.mean((MA.truth > 0.5) == (probs_2d_orig > 0.5),axis=0) )/ 10\n",
    "        pd.DataFrame( {'accuracy':accuracy, 'pathologies':Info.pathologies} ).set_index('pathologies').to_csv(path)\n",
    "        mlflow.log_artifact(path,artifact_path=f'accuracies/{dataset}/')\n",
    "\n",
    "        path = f'../../accuracy_aug_{dataset}.csv'\n",
    "        accuracy = np.floor( 1000*np.mean((MA.truth > 0.5) == (MA.probs_avg_2d > 0.5),axis=0) )/ 10\n",
    "        pd.DataFrame( {'accuracy':accuracy, 'pathologies':Info.pathologies} ).set_index('pathologies').to_csv(path)\n",
    "        mlflow.log_artifact(path,artifact_path=f'accuracies/{dataset}/')\n",
    "\n",
    "\n",
    "        \n",
    "    probs_2d_orig, final_results, MA = funcs.apply_technique_aim_1_2( how_to_treat_nans   = 'ignore', \n",
    "                                                                      data_generator      = Data.generator[dataset], \n",
    "                                                                      data_generator_aug  = Data.generator[dataset + '_aug'], \n",
    "                                                                      model               = model, \n",
    "                                                                      uncertainty_type    = 'std', \n",
    "                                                                      number_augmentation = number_augmentation)\n",
    "\n",
    "    log_results(dataframe     = Data.dataframe[dataset], \n",
    "                probs_2d_orig = probs_2d_orig, \n",
    "                pathologies   = pathologies, \n",
    "                MA            = MA, \n",
    "                dataset       = dataset)\n",
    "\n",
    "\n",
    "def setting_up_gpu():\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto(inter_op_parallelism_threads=5, intra_op_parallelism_threads=5) # , device_count={\"GPU\":1, \"CPU\": 10})\n",
    "    # config.gpu_options.allow_growth = True  \n",
    "    # config.log_device_placement = True  \n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    return sess\n",
    "\n",
    "\n",
    "def mlflow_setting_up():\n",
    "\n",
    "    server, artifact = funcs.mlflow_settings()\n",
    "    mlflow.set_tracking_uri(server)\n",
    "\n",
    "\n",
    "    \"\"\" Creating/Setting the experiment\n",
    "        Line below should be commented if the experiment is already created\n",
    "        If kept commented during the first run of a new experiment, the set_experiment \n",
    "        will automatically create the new experiment with local artifact storage \"\"\"\n",
    "\n",
    "    experiment_name = 'expanding_dataset_aim1_2'\n",
    "    # mlflow.create_experiment(name=experiment_name, artifact_location=artifact)\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    # Starting the MLflow \n",
    "    run = mlflow.start_run(run_id='106f71e138174d8db44bc6c32f537066') # run_name; run_id\n",
    "    # mlflow.set_tag(f'mlflow.note.content',f'run_id: {run.info.run_id}')\n",
    "\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GPU set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sess = setting_up_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## creating a ssh-tunnel to server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "command     = 'ssh -N -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org &'\n",
    "ssh_session = subprocess.Popen('exec ' + command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MLflow set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow set up\n",
    "run = mlflow_setting_up()\n",
    "\n",
    "# Loading the optimization parameters aturomatically from keras\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving the Git commit  (only in Jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git commit hash 98d373195612770089edd30079d26c26fa71e2a8\n"
     ]
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_commit_hash = repo.head.object.hexsha\n",
    "print('git commit hash', git_commit_hash)\n",
    "mlflow.set_tag('mlflow.source.git.commit', git_commit_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reading Terminal Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "getting_inputs_via_terminal = False\n",
    "\n",
    "if getting_inputs_via_terminal: \n",
    "    epochs, batch_size, max_sample, architecture_name, number_augmentation = funcs.reading_terminal_inputs()\n",
    "else:                           \n",
    "    epochs, batch_size, max_sample, architecture_name, number_augmentation = 3, 40, 1000000, 'DenseNet121', 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sample-pruning\n",
      "train: (223414, 20)\n",
      "test: (234, 19)\n",
      "\n",
      "after sample-pruning\n",
      "train (certain): (124626, 21)\n",
      "train (uncertain): (5807, 21)\n",
      "valid: (31157, 21)\n",
      "test: (169, 20) \n",
      "\n",
      "Found 169 validated image filenames.\n",
      "Found 169 validated image filenames.\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:219: UserWarning: `classes` will be ignored given the class_mode=\"raw\"\n",
      "  warnings.warn('`classes` will be ignored given the class_mode=\"{}\"'\n"
     ]
    }
   ],
   "source": [
    "dataset    = 'chexpert' # nih chexpert\n",
    "dir        = '/groups/jjrodrig/projects/chest/dataset/' + dataset + '/'\n",
    "\n",
    "running_new_run = False\n",
    "\n",
    "if running_new_run:\n",
    "    Data, Info = load_data.load_chest_xray(dir=dir, dataset=dataset, batch_size=batch_size, mode='train_val', max_sample=max_sample)\n",
    "\n",
    "    mlflow.log_param('dataset'     , dataset)\n",
    "    mlflow.log_param('max_sample'  , max_sample)\n",
    "    mlflow.log_param('train count' , len(Data.generator['train'].filenames))\n",
    "    mlflow.log_param('valid count' , len(Data.generator['valid'].filenames))\n",
    "    mlflow.log_param('batch size'  , batch_size)\n",
    "\n",
    "else:\n",
    "    Data, Info = load_data.load_chest_xray(dir=dir, dataset=dataset, batch_size=batch_size, mode='test', max_sample=max_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimize_model = False\n",
    "\n",
    "if optimize_model:\n",
    "    model = funcs.optimize( train_dataset     = Data.data_tf['train'], \n",
    "                            valid_dataset     = Data.data_tf['valid'], \n",
    "                            architecture_name = architecture_name,\n",
    "                            epochs            = epochs, \n",
    "                            Info              = Info,\n",
    "                            dir               = dir)\n",
    "else:\n",
    "    model = mlflow.keras.load_model(model_uri=f'runs:/{run.info.run_id}/model',compile=False)\n",
    "\n",
    "    model.compile(  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                    loss      = funcs.weighted_bce_loss(Info.class_weights), # tf.keras.losses.binary_crossentropy #  \n",
    "                    metrics   = [tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]running the evaluation on original non-augmented data\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.05it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s] running the evaluation on augmented data including the uncertainty measurement\n",
      "augmentation 0/3\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]augmentation 1/3\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.09it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]augmentation 2/3\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "EVALUATE = True\n",
    "\n",
    "if EVALUATE:\n",
    "\n",
    "    # validation dataset\n",
    "    running_evaluation( dataset             = 'valid', \n",
    "                        pathologies         = Info.pathologies, \n",
    "                        Data                = Data, \n",
    "                        model               = model, \n",
    "                        number_augmentation = number_augmentation)\n",
    "\n",
    "    # test dataset\n",
    "    Data, Info= load_data.load(dir=dir, dataset=dataset, batch_size=batch_size, mode='test', max_sample=max_sample)\n",
    "\n",
    "    running_evaluation( dataset             = 'test', \n",
    "                        pathologies         = Info.pathologies, \n",
    "                        Data                = Data, \n",
    "                        model               = model, \n",
    "                        number_augmentation = number_augmentation)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = 'test'\n",
    "# probs_2d_orig, final_results, MA = funcs.apply_technique_aim_1_2( how_to_treat_nans   = 'ignore', \n",
    "#                                                                     data_generator      = Data.generator[dataset], \n",
    "#                                                                     data_generator_aug  = Data.generator[dataset + '_aug'], \n",
    "#                                                                     model               = model, \n",
    "#                                                                     uncertainty_type    = 'std', \n",
    "#                                                                     number_augmentation = number_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def add_dataframe_info_columns(df_info, probs_2d, pathologies):\n",
    "\n",
    "#     df              = df_info.drop(pathologies,axis=1)\n",
    "#     df_temp         = pd.DataFrame(probs_2d_orig, columns=pathologies).set_index(df.index)\n",
    "#     df[pathologies] = df_temp[pathologies]\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# pathologies = Info.pathologies\n",
    "\n",
    "# path = f'../../prob_{dataset}.csv'\n",
    "# df = add_dataframe_info_columns(df_info=Data.dataframe[dataset], probs_2d=probs_2d_orig, pathologies=pathologies)       \n",
    "# df.to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path=f'probabilities/{dataset}/')\n",
    "\n",
    "# path = f'../../prob_aug_avg_{dataset}.csv'\n",
    "# pd.DataFrame( MA.probs_avg_2d, columns=Info.pathologies ).to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path=f'probabilities/{dataset}/')\n",
    "\n",
    "# path = f'../../uncertainty_{dataset}.csv'\n",
    "# pd.DataFrame( MA.probs_std_2d, columns=Info.pathologies ).to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path=f'uncertainties/{dataset}/')\n",
    "\n",
    "\n",
    "# path = f'../../accuracy_orig_{dataset}.csv'\n",
    "# accuracy = np.floor( 1000*np.mean((MA.truth > 0.5) == (probs_2d_orig > 0.5),axis=0) )/ 10\n",
    "# pd.DataFrame( {'accuracy':accuracy, 'pathologies':Info.pathologies} ).set_index('pathologies').to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path=f'accuracies/{dataset}/')\n",
    "\n",
    "# path = f'../../accuracy_aug_{dataset}.csv'\n",
    "# accuracy = np.floor( 1000*np.mean((MA.truth > 0.5) == (MA.probs_avg_2d > 0.5),axis=0) )/ 10\n",
    "# pd.DataFrame( {'accuracy':accuracy, 'pathologies':Info.pathologies} ).set_index('pathologies').to_csv(path)\n",
    "# mlflow.log_artifact(path,artifact_path=f'accuracies/{dataset}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Closing the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ending mlflow session\n",
      "Ending ssh session\n",
      "Optimization Complete\n"
     ]
    }
   ],
   "source": [
    "# End mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# End the ssh session. If this failed, we can type 'pkill ssh' in the terminal \n",
    "ssh_session.kill()\n",
    "\n",
    "print('Optimization Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('paper_miniforge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "97f50b47c5db4a373caba7d351ed0bd803d6a9b66b6e99b50d57389022e4f55d"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "fa8724247638d5e254d9d19df2232ec995d2e23674e16916912bd569c0dd30b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
