{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import funcs \n",
    "import load_data\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import subprocess\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "%reload_ext load_data\n",
    "%reload_ext funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def ssh_tunneling():\n",
    "    command = 'ssh -N -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org &'\n",
    "    ssh_session = subprocess.Popen('exec ' + command, stdout=subprocess.PIPE, shell=True)\n",
    "    return ssh_session\n",
    "\n",
    "def mlflow_settings():\n",
    "    \"\"\"\n",
    "    RUN UI with postgres and HPC:\n",
    "    REMOTE postgres server:\n",
    "        # connecting to remote server through ssh tunneling\n",
    "        ssh -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org\n",
    "\n",
    "        # using the mapped port and localhost to view the data\n",
    "        mlflow ui --backend-store-uri postgresql://artinmajdi:1234@localhost:5000/chest_db --port 6789\n",
    "\n",
    "    RUN directly from GitHub or show experiments/runs list:\n",
    "\n",
    "    export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n",
    "    \n",
    "    mlflow runs list --experiment-id <id>\n",
    "\n",
    "    mlflow run                 --no-conda --experiment-id 5 -P epoch=2 https://github.com/artinmajdi/mlflow_workflow.git -v main\n",
    "    mlflow run mlflow_workflow --no-conda --experiment-id 5 -P epoch=2\n",
    "    \n",
    "    PostgreSQL server style\n",
    "        server = f'{dialect_driver}://{username}:{password}@{ip}/{database_name}' \"\"\"\n",
    "\n",
    "    postgres_connection_type = { 'direct':     ('5432', 'data7-db1.cyverse.org'),\n",
    "                                    'ssh-tunnel': ('5000', 'localhost')\n",
    "                                }\n",
    "\n",
    "    port, host = postgres_connection_type['ssh-tunnel'] # 'direct' , 'ssh-tunnel'\n",
    "    username       = \"artinmajdi\"\n",
    "    password       = '1234'\n",
    "    database_name  = \"chest_db_v2\"\n",
    "    dialect_driver = 'postgresql'\n",
    "    server         = f'{dialect_driver}://{username}:{password}@{host}:{port}/{database_name}'\n",
    "\n",
    "    Artifacts = { 'hpc':        'sftp://mohammadsmajdi@filexfer.hpc.arizona.edu:/home/u29/mohammadsmajdi/projects/mlflow/artifact_store',\n",
    "                  'data7_db1':  'sftp://artinmajdi@data7-db1.cyverse.org:/home/artinmajdi/mlflow_data/artifact_store'} # temp2_data7_b\n",
    "\n",
    "   \n",
    "    return server, Artifacts['data7_db1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "# Creating/Setting the experiment\n",
    "experiment_name = 'soft_weighted_MV_aim1_3'\n",
    "\n",
    "# Line below should be commented if the experiment is already created\n",
    "# If kept commented during the first run of a new experiment, the set_experiment \n",
    "# will automatically create the new experiment with local artifact storage\n",
    "artifact = 'sftp://artinmajdi@data7-db1.cyverse.org:/home/artinmajdi/mlflow_data/artifact_store' # :temp2_data7_b\n",
    "\n",
    "if not client.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(name=experiment_name, artifact_location=artifact)\n",
    "\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_id='e98f3c431281497e8155d7384b11cca9'):\n",
    "    pass \n",
    "\n",
    "\n",
    "ssh_session.kill()\n",
    "\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicating the results without sftp password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# mlflow set-up\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# Creating/Setting the experiment\n",
    "# experiment_name_orig = 'label_inter_dependence'\n",
    "# experiment_name      = experiment_name_orig + '_npass'\n",
    "\n",
    "experiment_name = 'test_artifact2'\n",
    "\n",
    "artifact = 'sftp://artinmajdi:temp2_data7_b@data7-db1.cyverse.org:/home/artinmajdi/mlflow_data/artifact_store' # \n",
    "# mlflow.create_experiment(name=experiment_name, artifact_location=artifact)\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# # Starting the MLflow \n",
    "# with mlflow.start_run() as run:\n",
    "#     mlflow.log_artifact('../README.md')\n",
    "\n",
    "# # closing the ssh session\n",
    "# ssh_session.kill()\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='sftp://artinmajdi:temp2_data7_b@data7-db1.cyverse.org:/home/artinmajdi/mlflow_data/artifact_store', experiment_id='21', lifecycle_stage='active', name='label_inter_dependence', tags={}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "client.get_experiment_by_name(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<FileInfo: file_size=760, is_dir=False, path='README.md'>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_artifacts(run_id = '1e8808ab3d4d498482157580f2ef6f02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the info for an existing mlflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startin the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# finding the experiment id\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(\"<experiment_name>\").experiment_id\n",
    "\n",
    "# listing all runs\n",
    "mlflow.list_run_infos(experiment_id=experiment_id)\n",
    "new = mlflow.get_run('4e0ad8d3fb2b45b38b415756976c5909')\n",
    "\n",
    "# mlflow.list_run_infos(experiment_id=19)\n",
    "# full_training_but_local_artifact = mlflow.get_run('3b73b288158c4140bae14f7140a0b8aa')\n",
    "\n",
    "# closing the mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an mlflow run from old runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../../temp_without_unc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7155e2b1bb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mlocal_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../temp_without_unc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../../temp_without_unc'"
     ]
    }
   ],
   "source": [
    "\n",
    "# starting the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# # downloading the source artifacts\n",
    "experiment_name = 'expanding_dataset_aim1_2'\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "run_id_source = '9d766ecdaa0a4281a319076dfb30eda6'\n",
    "session_source = mlflow.get_run(run_id=run_id_source)\n",
    "\n",
    "\n",
    "session_parent = mlflow.start_run(run_id='45512fa086574aef99fb49eaa2239ed8') # run_name='effect of adding uncertain samples to dataset - whole dataset')\n",
    "# mlflow.set_tag(f'mlflow.note.content',f'run_id: {session_parent.info.run_id}')\n",
    "\n",
    "\n",
    "session_new = mlflow.start_run(run_name='with uncertain sampels - whole dataset', nested=True)\n",
    "mlflow.set_tag(f'mlflow.note.content',f'run_id: {session_new.info.run_id}')\n",
    "\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "local_dir = '../../temp_without_unc'\n",
    "os.mkdir(local_dir)\n",
    "full_path = client.download_artifacts(run_id=run_id_source, path='', dst_path=local_dir)\n",
    "\n",
    "# run = mlflow.active_run()\n",
    "\n",
    "# logging the parameters, metrics, artifacts, and tags\n",
    "mlflow.log_params(session_source.data.params)\n",
    "mlflow.log_metrics(session_source.data.metrics)\n",
    "mlflow.log_artifact(full_path + 'model',artifact_path='')\n",
    "# mlflow.log_artifact(full_path + 'model_summary.txt',artifact_path='')\n",
    "\n",
    "\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "mlflow.set_tag('mlflow.source.git.commit', repo.head.object.hexsha)\n",
    "mlflow.set_tag('mlflow.source.name'      , session_source.data.tags['mlflow.source.name'])\n",
    "\n",
    "\n",
    "\n",
    "# tf.keras.models.load_model()\n",
    "# model = mlflow.keras.load_model(model_uri=f'runs:/{run_id_parent}/model',compile=False)\n",
    "# mlflow.keras.log_artifact(model,artifact_path='',conda_env='../conda.yaml')\n",
    "\n",
    "\n",
    "#  Writing on top of the page of run\n",
    "mlflow.set_tag(f'mlflow.note.content',f'run_id: {session_new.info.run_id}')\n",
    "\n",
    "\n",
    "# closing the mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/tf2_4/lib/python3.9/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicating a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# starting the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# creating the experiment\n",
    "experiment_name = 'expanding_dataset_aim1_2'\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# downloading the source artifacts\n",
    "run_id_parent = 'bc306d0c76b94e19845f442f143fd5df'\n",
    "\n",
    "old_run = mlflow.get_run(run_id_parent)\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "local_dir = '../../temp_duplicate3' \n",
    "os.mkdir(local_dir)\n",
    "full_path = client.download_artifacts(run_id_parent, '', local_dir)\n",
    "\n",
    "\n",
    "run = mlflow.start_run(run_name='with uncertain sampels - whole dataset')\n",
    "\n",
    "mlflow.set_tag(f'mlflow.note.content',f'run_id: {run.info.run_id}')\n",
    "\n",
    "# logging the parameters, metrics, artifacts, and tags\n",
    "mlflow.log_params(old_run.data.params)\n",
    "mlflow.log_metrics(old_run.data.metrics)\n",
    "\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "mlflow.set_tag('mlflow.source.git.commit', repo.head.object.hexsha)\n",
    "mlflow.set_tag('mlflow.source.name'      , old_run.data.tags['mlflow.source.name'])\n",
    "mlflow.set_tag('mlflow.log-model.history', old_run.data.tags['mlflow.log-model.history'])\n",
    "\n",
    "\n",
    "model = mlflow.keras.load_model(model_uri=f'runs:/{run_id_parent}/model',compile=False)\n",
    "mlflow.keras.log_model(model,artifact_path='model',conda_env='../conda.yaml')\n",
    "\n",
    "# mlflow.log_artifact(full_path + 'model',artifact_path='')\n",
    "# mlflow.log_artifact(full_path + 'model_summary.txt',artifact_path='')\n",
    "# mlflow.log_artifact(full_path + 'conda.yaml',artifact_path='')\n",
    "\n",
    "# mlflow.log_artifact(full_path + 'train_val_142_samples',artifact_path='')\n",
    "# mlflow.log_artifact(full_path + 'train_val_full',artifact_path='')\n",
    "# mlflow.log_artifact(full_path + 'test',artifact_path='')\n",
    "\n",
    "\n",
    "\n",
    "# closing the mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# # closing the ssh session\n",
    "ssh_session.kill()\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startin the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# Downloading the artifact\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "run_id = '468f7bb48d4244dd8ebb7b5885e89d28'\n",
    "local_dir = '/home/u29/mohammadsmajdi/projects/chest_xray/'\n",
    "artifact_name = 'test_results.json'\n",
    "full_path = client.download_artifacts(run_id, artifact_name, local_dir)\n",
    "\n",
    "# loading the json file\n",
    "score = pd.read_json(full_path)\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resuming an existing mlflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'chexpert'\n",
    "dir = '/groups/jjrodrig/projects/chest/dataset/' + dataset + '/'\n",
    "\n",
    "# startin the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "mlflow.set_experiment(experiment_name='expanding_dataset_aim1_2')\n",
    "mlflow.start_run(run_id='45512fa086574aef99fb49eaa2239ed8')\n",
    "\n",
    "full_path  = '/home/u29/mohammadsmajdi/'\n",
    "mlflow.log_artifact(full_path + 'accuracy comparisons.xlsx',artifact_path='')\n",
    "mlflow.log_artifact(full_path + 'test.csv',artifact_path='')\n",
    "mlflow.log_params({'max_sample':1000000,'architecture_name':'DenseNet121', 'batch_size':50,'epochs':3,'number_augmentation':3,'learning_rate':0.001})\n",
    "\n",
    "\n",
    "# closing the mlflow session\n",
    "mlflow.end_run()\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Git commit  (only in Jupyter notebook)\n",
    "This is only needed for jupyter notebook\n",
    "\n",
    "You can annotate runs with arbitrary tags. Tag keys that start with mlflow. are reserved for internal use. The following tags are set automatically by MLflow, when appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_commit_hash = repo.head.object.hexsha\n",
    "print('git commit hash', git_commit_hash)\n",
    "mlflow.set_tag('mlflow.source.git.commit', git_commit_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startin the ssh session\n",
    "ssh_session = ssh_tunneling()\n",
    "\n",
    "# setting the tracking uri\n",
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "# creating the experiment\n",
    "# mlflow.create_experiment(name='label_inter_dependence', artifact_location=artifact)\n",
    "mlflow.set_experiment(experiment_name='label_inter_dependence')\n",
    "\n",
    "# Create nested runs\n",
    "with mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n",
    "    mlflow.log_param(\"parent\", \"yes\")\n",
    "    with mlflow.start_run(run_name='CHILD 1', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", 1)\n",
    "\n",
    "    with mlflow.start_run(run_name='CHILD 2', nested=True) as child_run:\n",
    "        mlflow.log_param(\"child\", 2)\n",
    "\n",
    "\n",
    "# closing the ssh session\n",
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the parent info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the experiment id\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_name = 'expanding_dataset_aim1_2'\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# getting the parent session info\n",
    "mlflow.list_run_infos(experiment_id=experiment_id)\n",
    "run_id_parent   = '329102d83efe4586a307bac05c92c298'\n",
    "parent_session = mlflow.get_run(run_id_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnnig an mlflow project from github\n",
    "Run MLflow project and create a reproducible conda environment on a local host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "github_repo = \"https://github.com/mlflow/mlflow-example\"\n",
    "params = {\"max_sample\": 2000,\"epoch\": 1}\n",
    "\n",
    "\n",
    "mlflow.run(uri=github_repo, parameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the model from remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  models:/<model_name>/<model_version>\n",
    "model = mlflow.keras.load_model(model_uri='models:/Chexpert-whole-dataset/1',compile=False)\n",
    "\n",
    "# models:/<model_name>/<stage> \n",
    "model = mlflow.keras.load_model(model_uri='models:/Chexpert-whole-dataset/production',compile=False)\n",
    "\n",
    "#  runs:/<mlflow_run_id>/run-relative-path-to-model\n",
    "run_id = 'f7d6e3b515da4ed89578cdd53412fcf8'\n",
    "model = mlflow.keras.load_model(model_uri='runs:/{}/model'.format(run_id),compile=False)\n",
    "\n",
    "# /Users/me/path/to/local/model\n",
    "model = mlflow.keras.load_model(model_uri='/home/u29/mohammadsmajdi/projects/chest_xray/artifacts_optimized_model/model',compile=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the artifact from remote server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downloading the test results\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "local_dir = '../../'\n",
    "full_path = client.download_artifacts(run_id=run_id_parent, path='test_results.json', dst_path=local_dir)\n",
    "\n",
    "# Loading the downloaded json file\n",
    "score = pd.read_json(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs( experiment_id, 'params.max_sample > 20000' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_miniforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "metadata": {
   "interpreter": {
    "hash": "97f50b47c5db4a373caba7d351ed0bd803d6a9b66b6e99b50d57389022e4f55d"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
