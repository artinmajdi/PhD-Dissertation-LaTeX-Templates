
@misc{noauthor_google_nodate,
	title = {Google {Colaboratory}},
	url = {https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb#scrollTo=P8fDpUyl75pT},
	language = {en},
	urldate = {2022-06-23},
}

@article{hou_few-shot_2019,
	title = {Few-shot sequence labeling with label dependency transfer and pair-wise embedding},
	journal = {arXiv preprint arXiv:1906.08711},
	author = {Hou, Yutai and Zhou, Zhihan and Liu, Yijia and Wang, Ning and Che, Wanxiang and Liu, Han and Liu, Ting},
	year = {2019},
	keywords = {‚õî No DOI found},
}

@inproceedings{zhao_hot-vae_2021,
	title = {{HOT}-{VAE}: {Learning} {High}-{Order} {Label} {Correlation} for {Multi}-{Label} {Classification} via {Attention}-{Based} {Variational} {Autoencoders}},
	shorttitle = {{HOT}-{VAE}},
	abstract = {A novel framework for multi-label classification, Highorder Tie-in Variational Autoencoder (HOT-VAE), which performs adaptive high-order label correlation learning, which outperforms the existing state-of-the-art approaches on a bird distribution dataset. Understanding how environmental characteristics affect biodiversity patterns, from individual species to communities of species, is critical for mitigating effects of global change. A central goal for conservation planning and monitoring is the ability to accurately predict the occurrence of species communities and how these communities change over space and time. This in turn leads to a challenging and long-standing problem in the field of computer science how to perform accurate multi-label classification with hundreds of labels? The key challenge of this problem is its exponential-sized output space with regards to the number of labels to be predicted. Therefore, it is essential to facilitate the learning process by exploiting correlations (or dependency) among labels. Previous methods mostly focus on modelling the correlation on label pairs; however, complex relations between real-world objects often go beyond second order. In this paper, we propose a novel framework for multi-label classification, Highorder Tie-in Variational Autoencoder (HOT-VAE), which performs adaptive high-order label correlation learning. We experimentally verify that our model outperforms the existing state-of-the-art approaches on a bird distribution dataset on both conventional F1 scores and a variety of ecological metrics. To show our method is general, we also perform empirical analysis on seven other public real-world datasets in several application domains, and Hot-VAE exhibits superior performance to previous methods.},
	booktitle = {{AAAI}},
	author = {Zhao, Wenting and Kong, Shufeng and Bai, Junwen and Fink, Daniel and Gomes, Carla P.},
	year = {2021},
	keywords = {‚õî No DOI found},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Large}-scale {Robust} {Deep} {AUC} {Maximization}: {A} {New} {Surrogate} {Loss} and {Empirical} {Studies} on {Medical} {Image} {Classification}},
	shorttitle = {Papers with {Code} - {Large}-scale {Robust} {Deep} {AUC} {Maximization}},
	url = {https://paperswithcode.com/paper/robust-deep-auc-maximization-a-new-surrogate},
	abstract = {üèÜ SOTA for Multi-Label Classification on CheXpert (AVERAGE AUC ON 14 LABEL metric)},
	language = {en},
	urldate = {2022-06-23},
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {CheXpert} {Benchmark} ({Multi}-{Label} {Classification})},
	url = {https://paperswithcode.com/sota/multi-label-classification-on-chexpert},
	abstract = {The current state-of-the-art on CheXpert is DeepAUC-v1. See a full comparison of 205 papers with code.},
	language = {en},
	urldate = {2022-06-23},
}

@article{burkhardt_online_2018,
	title = {Online multi-label dependency topic models for text classification},
	volume = {107},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-017-5689-6},
	doi = {10.1007/s10994-017-5689-6},
	abstract = {Multi-label text classification is an increasingly important field as large amounts of text data are available and extracting relevant information is important in many application contexts. Probabilistic generative models are the basis of a number of popular text mining methods such as Naive Bayes or Latent Dirichlet Allocation. However, Bayesian models for multi-label text classification often are overly complicated to account for label dependencies and skewed label frequencies while at the same time preventing overfitting. To solve this problem we employ the same technique that contributed to the success of deep learning in recent years: greedy layer-wise training. Applying this technique in the supervised setting prevents overfitting and leads to better classification accuracy. The intuition behind this approach is to learn the labels first and subsequently add a more abstract layer to represent dependencies among the labels. This allows using a relatively simple hierarchical topic model which can easily be adapted to the online setting. We show that our method successfully models dependencies online for large-scale multi-label datasets with many labels and improves over the baseline method not modeling dependencies. The same strategy, layer-wise greedy training, also makes the batch variant competitive with existing more complex multi-label topic models.},
	language = {en},
	number = {5},
	urldate = {2022-06-23},
	journal = {Machine Learning},
	author = {Burkhardt, Sophie and Kramer, Stefan},
	month = may,
	year = {2018},
	keywords = {LDA, Multi-label classification, Online learning, Topic model},
	pages = {859--886},
}

@article{burkhardt_online_2018-1,
	title = {Online multi-label dependency topic models for text classification},
	volume = {107},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-017-5689-6},
	doi = {10.1007/s10994-017-5689-6},
	abstract = {Multi-label text classification is an increasingly important field as large amounts of text data are available and extracting relevant information is important in many application contexts. Probabilistic generative models are the basis of a number of popular text mining methods such as Naive Bayes or Latent Dirichlet Allocation. However, Bayesian models for multi-label text classification often are overly complicated to account for label dependencies and skewed label frequencies while at the same time preventing overfitting. To solve this problem we employ the same technique that contributed to the success of deep learning in recent years: greedy layer-wise training. Applying this technique in the supervised setting prevents overfitting and leads to better classification accuracy. The intuition behind this approach is to learn the labels first and subsequently add a more abstract layer to represent dependencies among the labels. This allows using a relatively simple hierarchical topic model which can easily be adapted to the online setting. We show that our method successfully models dependencies online for large-scale multi-label datasets with many labels and improves over the baseline method not modeling dependencies. The same strategy, layer-wise greedy training, also makes the batch variant competitive with existing more complex multi-label topic models.},
	language = {en},
	number = {5},
	urldate = {2022-06-19},
	journal = {Machine Learning},
	author = {Burkhardt, Sophie and Kramer, Stefan},
	month = may,
	year = {2018},
	keywords = {LDA, Multi-label classification, Online learning, Topic model},
	pages = {859--886},
}

@article{dembczynski_label_2012,
	title = {On label dependence and loss minimization in multi-label classification},
	volume = {88},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-012-5285-8},
	doi = {10.1007/s10994-012-5285-8},
	abstract = {Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Comparing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than proposing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the predictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two representative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to reveal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.},
	language = {en},
	number = {1},
	urldate = {2022-06-23},
	journal = {Machine Learning},
	author = {Dembczy≈Ñski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H√ºllermeier, Eyke},
	month = jul,
	year = {2012},
	keywords = {Label dependence, Loss functions, Multi-label classification},
	pages = {5--45},
}

@article{kim_multilabel_2020,
	title = {Multilabel na√Øve {Bayes} classification considering label dependence},
	volume = {136},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865520302397},
	doi = {10.1016/j.patrec.2020.06.021},
	abstract = {Multilabel classification is the task of assigning relevant labels to an instance, and it has received considerable attention in recent years. This task can be performed by extending a single-label classifier, such as the na√Øve Bayes classifier, to utilize the useful relations among labels for achieving better multilabel classification accuracy. However, the conventional multilabel na√Øve Bayes classifier treats each label independently and hence neglects the relations among labels, resulting in degenerated accuracy. We propose a new multilabel na√Øve Bayes classifier that considers the relations or dependence among labels. Experimental results show that the proposed method outperforms conventional multilabel classifiers.},
	language = {en},
	urldate = {2022-06-23},
	journal = {Pattern Recognition Letters},
	author = {Kim, Hae-Cheon and Park, Jin-Hyeong and Kim, Dae-Won and Lee, Jaesung},
	month = aug,
	year = {2020},
	keywords = {Label dependence, Multilabel classifier, Na√Øve Bayes classification},
	pages = {279--285},
}

@article{he_multilabel_2020,
	title = {Multilabel classification by exploiting data-driven pair-wise label dependence},
	volume = {35},
	issn = {1098-111X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22257},
	doi = {10.1002/int.22257},
	abstract = {Exploiting label dependence is a widely used approach to boost classification performance for multilabel classification problems. However, most of the traditional label dependence methods have high time complexity, especially when combined with deep neural networks (DNNs). Thus they usually can not be efficiently applied in large-scale data sets. Recent advances in large-scale multilabel classification widely developed pair-wise ranking and structure-driven methods, but label dependence was little exploited. In most of the structure-driven methods, binary relevance (BR) with multiple binary cross-entropy (BCE) loss functions, a simple but effective method, is still the prior solution incorporation with DNNs in large-scale data sets. In this paper, we propose a novel loss function called label dependent cross-entropy (LDCE), which directly introduces label dependence to BCE loss function by data-driven conditional probability. Combined with deep convolutional neural networks (DCNNs), LDCE introduces no extra parameters and induces very little extra computational complexity. Moreover, we develop its tiny variant with sparse label dependence and its learnable version for automatic learning pair-wise label dependence. Within the BR scheme, LDCE outperforms BCE on seven widely used benchmark datasets. We also perform two large-scale multilabel image classification tasks (VOC 2007 and ChestX-ray14) with DCNNs, and LDCE outperforms BCE and achieves comparable results to the state-of-the-art.},
	language = {en},
	number = {9},
	urldate = {2022-06-23},
	journal = {International Journal of Intelligent Systems},
	author = {He, Tao and Zhang, Lei and Guo, Jixiang and Yi, Zhang},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22257},
	keywords = {cross-entropy, data-driven, deep neural networks, label dependent, multi-label classification, pair-wise label dependence},
	pages = {1375--1396},
}

@inproceedings{zhang_multi-modal_2020,
	address = {Online},
	title = {Multi-modal {Multi}-label {Emotion} {Detection} with {Modality} and {Label} {Dependence}},
	url = {https://aclanthology.org/2020.emnlp-main.291},
	doi = {10.18653/v1/2020.emnlp-main.291},
	abstract = {As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach.},
	urldate = {2022-06-23},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Dong and Ju, Xincheng and Li, Junhui and Li, Shoushan and Zhu, Qiaoming and Zhou, Guodong},
	month = nov,
	year = {2020},
	pages = {3584--3593},
}

@inproceedings{el-fiky_multi-label_2021,
	title = {Multi-{Label} {Transfer} {Learning} for {Identifying} {Lung} {Diseases} using {Chest} {X}-{Rays}},
	doi = {10.1109/ICEEM52022.2021.9480622},
	abstract = {Chest radiography presents one of the main medical imaging modalities for diagnosing lung diseases. To assist radiologists during interventional procedures, this paper aims at proposing a transfer learning-based classifier to automatically identify 14 different thoracic diseases in Chest X-ray (CXR) images. The proposed method is relied on deep residual neural networks with 50 layers (ResNet-50) to accomplish the diagnostic task of many chest diseases. In this study, a public dataset of 112,120 frontal radiograph images for Chest X-ray has been used for validating the proposed deep learning classifier. It achieved the best performance of multi-label classification of normal and 14 different lung diseases with an average area under curve (AUC) of 0.911 and F1-score of 0.66. This study demonstrated that the proposed ResNet-50 classifier as a transfer learning model outperforms other relevant methods in the previous studies for automatic multi-label classification of chest X-rays.},
	booktitle = {2021 {International} {Conference} on {Electronic} {Engineering} ({ICEEM})},
	author = {El-Fiky, Azza and Shouman, Marwa Ahmed and Hamada, Salwa and El-Sayed, Ayman and Karar, Mohamed Esmail},
	month = jul,
	year = {2021},
	keywords = {Biomedical imaging, Computer-aided diagnosis, Deep learning, Diagnostic radiography, Pulmonary diseases, Task analysis, Thorax diseases, Transfer learning, X-ray, X-ray imaging, classification},
	pages = {1--6},
}

@article{yap_multi-label_2021,
	title = {Multi-label classification and label dependence in in silico toxicity prediction},
	volume = {74},
	issn = {0887-2333},
	url = {https://www.sciencedirect.com/science/article/pii/S0887233321000825},
	doi = {10.1016/j.tiv.2021.105157},
	abstract = {Most computational predictive models are specifically trained for a single toxicity endpoint and lack the ability to learn dependencies between endpoints, such as those targeting similar biological pathways. In this study, we compare the performance of 3 multi-label classification (MLC) models, namely Classifier Chains (CC), Label Powersets (LP) and Stacking (SBR), against independent classifiers (Binary Relevance) on Tox21 challenge data. Also, we develop a novel label dependence measure that shows full range of values, even at low prior probabilities, for the purpose of data-driven label partitioning. Using Logistic Regression as the base classifier and random label partitioning (k¬†=¬†3), CC show statistically significant improvements in model performance using Hamming and multi-label accuracy scores (p{\textless}0.05), while SBR show significant improvements in multi-label accuracy scores. The weights in the Logistic Regression and Stacking models are positively associated with label dependencies, suggesting that learning label dependence is a key contributor to improving model performance. An original quantitative measure of label dependency is combined with the Louvain community detection method to learn label partitioning using a data-driven process. The resulting MLCs with learned label partitioning were generally found to be non-inferior to their corresponding random or no label partitioning counterparts. Additionally, using the Random Forest classifier in a 10-fold stratified cross validation Stacking model, we find that the top-performing stacking model out-performs the corresponding base model in 11 out of 12 Tox21 labels. Taken together, these results suggest that MLC models could potentially boost the performance of current single-endpoint predictive models and that label partitioning learning may be used in place of random label partitionings.},
	language = {en},
	urldate = {2022-06-23},
	journal = {Toxicology in Vitro},
	author = {Yap, Xiu Huan and Raymer, Michael},
	month = aug,
	year = {2021},
	keywords = {Label dependence, Multi-label classification, Tox21, Toxicity prediction},
	pages = {105157},
}

@misc{yuan_large-scale_2021,
	title = {Large-scale {Robust} {Deep} {AUC} {Maximization}: {A} {New} {Surrogate} {Loss} and {Empirical} {Studies} on {Medical} {Image} {Classification}},
	shorttitle = {Large-scale {Robust} {Deep} {AUC} {Maximization}},
	url = {http://arxiv.org/abs/2012.03173},
	abstract = {Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org).},
	urldate = {2022-06-23},
	publisher = {arXiv},
	author = {Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},
	month = sep,
	year = {2021},
	note = {Number: arXiv:2012.03173
arXiv:2012.03173 [cs, math, stat]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{zhang_multi-label_2010,
	address = {New York, NY, USA},
	series = {{KDD} '10},
	title = {Multi-label learning by exploiting label dependency},
	isbn = {978-1-4503-0055-1},
	url = {https://doi.org/10.1145/1835804.1835930},
	doi = {10.1145/1835804.1835930},
	abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the conditional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
	urldate = {2022-06-19},
	booktitle = {Proceedings of the 16th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Min-Ling and Zhang, Kun},
	month = jul,
	year = {2010},
	keywords = {bayesian network, machine learning, multi-label learning},
	pages = {999--1008},
}

@misc{pham_interpreting_2020,
	title = {Interpreting chest {X}-rays via {CNNs} that exploit hierarchical disease dependencies and uncertainty labels},
	url = {http://arxiv.org/abs/1911.06475},
	abstract = {Chest radiography is one of the most common types of diagnostic radiology exams, which is critical for screening and diagnosis of many different thoracic diseases. Specialized algorithms have been developed to detect several specific pathologies such as lung nodule or lung cancer. However, accurately detecting the presence of multiple diseases from chest X-rays (CXRs) is still a challenging task. This paper presents a supervised multi-label classification framework based on deep convolutional neural networks (CNNs) for predicting the risk of 14 common thoracic diseases. We tackle this problem by training state-of-the-art CNNs that exploit dependencies among abnormality labels. We also propose to use the label smoothing technique for a better handling of uncertain samples, which occupy a significant portion of almost every CXR dataset. Our model is trained on over 200,000 CXRs of the recently released CheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in predicting 5 selected pathologies from the validation set. This is the highest AUC score yet reported to date. The proposed method is also evaluated on the independent test set of the CheXpert competition, which is composed of 500 CXR studies annotated by a panel of 5 experienced radiologists. The performance is on average better than 2.6 out of 3 other individual radiologists with a mean AUC of 0.930, which ranks first on the CheXpert leaderboard at the time of writing this paper.},
	urldate = {2022-06-23},
	publisher = {arXiv},
	author = {Pham, Hieu H. and Le, Tung T. and Tran, Dat Q. and Ngo, Dat T. and Nguyen, Ha Q.},
	month = jun,
	year = {2020},
	note = {Number: arXiv:1911.06475
arXiv:1911.06475 [cs, eess]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{ge_improving_2020,
	title = {Improving multi-label chest {X}-ray disease diagnosis by exploiting disease and health labels dependencies},
	volume = {79},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-08260-2},
	doi = {10.1007/s11042-019-08260-2},
	abstract = {The widely used ChestX-ray14 dataset addresses an important medical image classification problem and has the following caveats: 1) many lung pathologies are visually similar, 2) a variant of multiple diseases including lung cancer, tuberculosis, and pneumonia are present in a single scan at the same time, i.e. multiple labels. Existing literature uses state-of-the-art deep learning models being transfer learned where output neurons of the networks are trained for individual diseases to cater for multiple disease labels in each image. However, most of them don‚Äôt consider the label relationship explicitly between present and absent classes. In this work we have proposed a pair of novel error functions that can be employed for any deep learning model, Multi-label Softmax Loss (MSML) and Correlation Loss (CorLoss), to specifically address the properties of multiple labels and visually similar data. Moreover, we provide a fine-grained perspective into this problem and use bilinear pooling as an encoding scheme to increase discrimination of the model. The experiments are conducted on the ChestX-ray14 dataset. We first report improvements using our proposed loss with various backbone networks. After that, we extend our experiments to prove the rich disparity being learned by the model with our proposed losses, which can be fused with other models to improve the overall performances.},
	language = {en},
	number = {21},
	urldate = {2022-06-23},
	journal = {Multimedia Tools and Applications},
	author = {Ge, Zongyuan and Mahapatra, Dwarikanath and Chang, Xiaojun and Chen, Zetao and Chi, Lianhua and Lu, Huimin},
	month = jun,
	year = {2020},
	keywords = {Chest X-Ray disease recognition, Deep convolutional neural network, Model fusion, Multi-label learning},
	pages = {14889--14902},
}

@article{baltruschat_comparison_2019,
	title = {Comparison of {Deep} {Learning} {Approaches} for {Multi}-{Label} {Chest} {X}-{Ray} {Classification}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-42294-8},
	doi = {10.1038/s41598-019-42294-8},
	abstract = {The increased availability of labeled X-ray image archives (e.g. ChestX-ray14 dataset) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolution of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a concluding experiment, we also investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we compare the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. Overall, we observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-38, integrating non-image data yields the best overall results. Furthermore, class activation maps are used to understand the classification process, and a detailed analysis of the impact of non-image features is provided.},
	language = {en},
	number = {1},
	urldate = {2022-06-23},
	journal = {Scientific Reports},
	author = {Baltruschat, Ivo M. and Nickisch, Hannes and Grass, Michael and Knopp, Tobias and Saalbach, Axel},
	month = apr,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Pathology, Radiography},
	pages = {6381},
}

@inproceedings{chen_deep_2019,
	title = {Deep {Hierarchical} {Multi}-label {Classification} of {Chest} {X}-ray {Images}},
	url = {https://proceedings.mlr.press/v102/chen19a.html},
	abstract = {Chest X-rays (CXRs) are a crucial and extraordinarily common diagnostic tool, leading to heavy research for Computer-Aided Diagnosis (CAD) solutions. However, both high classification accuracy and meaningful model predictions that respect and incorporate clinical taxonomies are crucial for CAD usability. To this end, we present a deep Hierarchical Multi-Label Classification (HMLC) approach for CXR CAD. Different than other hierarchical systems, we show that first training the network to model conditional probability directly and then refining it with unconditional probabilities is key in boosting performance. In addition, we also formulate a numerically stable cross-entropy loss function for unconditional probabilities that provides concrete performance improvements. To the best of our knowledge, we are the first to apply HMLC to medical imaging CAD. We extensively evaluate our approach on detecting 14 abnormality labels from the PLCO dataset, which comprises 198,000 manually annotated CXRs. We report a mean Area Under the Curve (AUC) of 0.887, the highest yet reported for this dataset. These performance improvements, combined with the inherent usefulness of taxonomic predictions, indicate that our approach represents a useful step forward for CXR CAD.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {Proceedings of {The} 2nd {International} {Conference} on {Medical} {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Chen, Haomin and Miao, Shun and Xu, Daguang and Hager, Gregory D. and Harrison, Adam P.},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {109--120},
}

@article{chen_deep_2020,
	title = {Deep hiearchical multi-label classification applied to chest {X}-ray abnormality taxonomies},
	volume = {66},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520301754},
	doi = {10.1016/j.media.2020.101811},
	abstract = {Chest X-rays (CXRs) are a crucial and extraordinarily common diagnostic tool, leading to heavy research for computer-aided diagnosis (CAD) solutions. However, both high classification accuracy and meaningful model predictions that respect and incorporate clinical taxonomies are crucial for CAD usability. To this end, we present a deep hierarchical multi-label classification (HMLC) approach for CXR CAD. Different than other hierarchical systems, we show that first training the network to model conditional probability directly and then refining it with unconditional probabilities is key in boosting performance. In addition, we also formulate a numerically stable cross-entropy loss function for unconditional probabilities that provides concrete performance improvements. Finally, we demonstrate that HMLC can be an effective means to manage missing or incomplete labels. To the best of our knowledge, we are the first to apply HMLC to medical imaging CAD. We extensively evaluate our approach on detecting abnormality labels from the CXR arm of the Prostate, Lung, Colorectal and Ovarian (PLCO) dataset, which comprises over 198,000 manually annotated CXRs. When using complete labels, we report a mean area under the curve (AUC) of 0.887, the highest yet reported for this dataset. These results are supported by ancillary experiments on the PadChest dataset, where we also report significant improvements, 1.2\% and 4.1\% in AUC and average precision, respectively over strong ‚Äúflat‚Äù classifiers. Finally, we demonstrate that our HMLC approach can much better handle incompletely labelled data. These performance improvements, combined with the inherent usefulness of taxonomic predictions, indicate that our approach represents a useful step forward for CXR CAD.},
	language = {en},
	urldate = {2022-06-23},
	journal = {Medical Image Analysis},
	author = {Chen, Haomin and Miao, Shun and Xu, Daguang and Hager, Gregory D. and Harrison, Adam P.},
	month = dec,
	year = {2020},
	keywords = {Chest X-ray, Computer aided diagnosis, Hierarchical multi-label classification},
	pages = {101811},
}

@misc{noauthor_cxr8_nodate,
	title = {{CXR8} {\textbar} {Powered} by {Box}},
	url = {https://nihcc.app.box.com/v/ChestXray-NIHCC},
	urldate = {2022-06-23},
}

@article{tsai_adversarial_2019,
	title = {Adversarial {Learning} of {Label} {Dependency}: {A} {Novel} {Framework} for {Multi}-class {Classification}},
	shorttitle = {Adversarial {Learning} of {Label} {Dependency}},
	doi = {10.1109/ICASSP.2019.8682549},
	abstract = {This work proposes a novel framework based on generative adversarial networks (GANs) to model label dependency and shows that the discriminator improves generalization ability for different kinds of models. Recent work has shown that exploiting relations between labels improves the performance of multi-label classification. We propose a novel framework based on generative adversarial networks (GANs) to model label dependency. The discriminator learns to model label dependency by discriminating real and generated label sets. To fool the discriminator, the classifier, or generator, learns to generate label sets with dependencies close to real data. Extensive experiments and comparisons on two large-scale image classification benchmark datasets (MS-COCO and NUS-WIDE) show that the discriminator improves generalization ability for different kinds of models.},
	journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Tsai, Che-Ping and Lee, Hung-yi},
	year = {2019},
}

@inproceedings{kumar_boosted_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Boosted {Cascaded} {Convnets} for {Multilabel} {Classification} of {Thoracic} {Diseases} in {Chest} {Radiographs}},
	isbn = {978-3-319-93000-8},
	doi = {10.1007/978-3-319-93000-8_62},
	abstract = {Chest X-ray is one of the most accessible medical imaging technique for diagnosis of multiple diseases. With the availability of ChestX-ray14, which is a massive dataset of chest X-ray images and provides annotations for 14 thoracic diseases; it is possible to train Deep Convolutional Neural Networks (DCNN) to build Computer Aided Diagnosis (CAD) systems. In this work, we experiment a set of deep learning models and present a cascaded deep neural network that can diagnose all 14 pathologies better than the baseline and is competitive with other published methods. Our work provides the quantitative results to answer following research questions for the dataset: (1) What loss functions to use for training DCNN from scratch on ChestX-ray14 dataset that demonstrates high class imbalance and label co occurrence? (2) How to use cascading to model label dependency and to improve accuracy of the deep learning model?},
	language = {en},
	booktitle = {Image {Analysis} and {Recognition}},
	publisher = {Springer International Publishing},
	author = {Kumar, Pulkit and Grewal, Monika and Srivastava, Muktabh Mayank},
	editor = {Campilho, Aur√©lio and Karray, Fakhri and ter Haar Romeny, Bart},
	year = {2018},
	keywords = {Boosted Cascade, Model Label Dependencies, Multilabel ClassiÔ¨Åcation, Thoracic Disease, Training DCNN},
	pages = {546--552},
}

@inproceedings{agu_anaxnet_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AnaXNet}: {Anatomy} {Aware} {Multi}-label {Finding} {Classification} in {Chest} {X}-{Ray}},
	isbn = {978-3-030-87240-3},
	shorttitle = {{AnaXNet}},
	doi = {10.1007/978-3-030-87240-3_77},
	abstract = {Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information. In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. Specifically, our model consists of two modules, the detection module and the anatomical dependency module. The latter utilizes graph convolutional networks, which enable our model to learn not only the label dependency but also the relationship between the anatomical regions in the chest X-ray. We further utilize a method to efficiently create an adjacency matrix for the anatomical regions using the correlation of the label across the different regions. Detailed experiments and analysis of our results show the effectiveness of our method when compared to the current state-of-the-art multi-label chest X-ray image classification methods while also providing accurate location information.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} ‚Äì {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Agu, Nkechinyere N. and Wu, Joy T. and Chao, Hanqing and Lourentzou, Ismini and Sharma, Arjun and Moradi, Mehdi and Yan, Pingkun and Hendler, James},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St√©phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	keywords = {Graph convolutional networks, Graph representation, Multi-label chest X-ray image classification},
	pages = {804--813},
}

@article{albahli_ai-driven_2021,
	title = {{AI}-driven deep {CNN} approach for multi-label pathology classification using chest {X}-{Rays}},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-495},
	doi = {10.7717/peerj-cs.495},
	abstract = {Artificial intelligence (AI) has played a significant role in image analysis and feature extraction, applied to detect and diagnose a wide range of chest-related diseases. Although several researchers have used current state-of-the-art approaches and have produced impressive chest-related clinical outcomes, specific techniques may not contribute many advantages if one type of disease is detected without the rest being identified. Those who tried to identify multiple chest-related diseases were ineffective due to insufficient data and the available data not being balanced. This research provides a significant contribution to the healthcare industry and the research community by proposing a synthetic data augmentation in three deep Convolutional Neural Networks (CNNs) architectures for the detection of 14 chest-related diseases. The employed models are DenseNet121, InceptionResNetV2, and ResNet152V2; after training and validation, an average ROC-AUC score of 0.80 was obtained competitive as compared to the previous models that were trained for multi-class classification to detect anomalies in x-ray images. This research illustrates how the proposed model practices state-of-the-art deep neural networks to classify 14 chest-related diseases with better accuracy.},
	language = {en},
	urldate = {2022-06-23},
	journal = {PeerJ Computer Science},
	author = {Albahli, Saleh and Rauf, Hafiz Tayyab and Algosaibi, Abdulelah and Balas, Valentina Emilia},
	month = apr,
	year = {2021},
	note = {Publisher: PeerJ Inc.},
	pages = {e495},
}

@article{allaouzi_novel_2019,
	title = {A {Novel} {Approach} for {Multi}-{Label} {Chest} {X}-{Ray} {Classification} of {Common} {Thorax} {Diseases}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2916849},
	abstract = {Chest X-ray (CXR) is one of the most common types of radiology examination for the diagnosis of thorax diseases. Computer-aided diagnosis (CAD) was developed to help radiologists to achieve diagnostic excellence in a short period of time and to enhance patient healthcare. In this paper, we seek to improve the performance of the CAD system in the task of thorax diseases diagnosis by providing a new method that combines the advantages of CNN models in image feature extraction with those of the problem transformation methods in the multi-label classification task. The experimental study is tested on two publicly available CXR datasets ChestX-ray14 (frontal view) and CheXpert (frontal and lateral views). The results show that our proposed method outperformed the current state of the art.},
	journal = {IEEE Access},
	author = {Allaouzi, Imane and Ben Ahmed, Mohamed},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Biomedical imaging, CAD, CNN, CXR, Diseases, Feature extraction, Image classification, Solid modeling, Task analysis, Thorax, computer vision, deep learning, image classification, image feature extraction, multi-label classification, problem transformation method, thoracic pathologies, transfer learning},
	pages = {64279--64288},
}

@article{priya_federated_2021,
	title = {A federated approach for detecting the chest diseases using {DenseNet} for multi-label classification},
	issn = {2198-6053},
	url = {https://doi.org/10.1007/s40747-021-00474-y},
	doi = {10.1007/s40747-021-00474-y},
	abstract = {Multi-label disease classification algorithms help to predict various chronic diseases at an early stage. Diverse deep neural networks are applied for multi-label classification problems to foresee multiple mutually non-exclusive classes or diseases. We propose a federated approach for detecting the chest diseases using DenseNets for better accuracy in prediction of various diseases. Images of chest X-ray from the Kaggle repository is used as the dataset in the proposed model. This new model is tested with both sample and full dataset of chest X-ray, and it outperforms existing models in terms of various evaluation metrics. We adopted transfer learning approach along with the pre-trained network from scratch to improve performance. For this, we have integrated DenseNet121 to our framework. DenseNets have a few focal points as they help to overcome vanishing gradient issues, boost up the feature propagation and reuse and also to reduce the number of parameters. Furthermore, gradCAMS are used as visualization methods to visualize the affected parts on chest X-ray. Henceforth, the proposed architecture will help the prediction of various diseases from a single chest X-ray and furthermore direct the doctors and specialists for taking timely decisions.},
	language = {en},
	urldate = {2022-06-23},
	journal = {Complex \& Intelligent Systems},
	author = {Priya, K. V. and Peter, J. Dinesh},
	month = jul,
	year = {2021},
	keywords = {Deep learning, Multi-label classification, Transfer learning},
}

@inproceedings{sekuboyina_relational-learning_2021,
	title = {A {Relational}-{Learning} {Perspective} {To} {Multi}-{Label} {Chest} {X}-{Ray} {Classification}},
	doi = {10.1109/ISBI48211.2021.9433786},
	abstract = {Multi-label classification of chest X-ray images is frequently performed using discriminative approaches, i.e. learning to map an image directly to its binary labels. Such approaches make it challenging to incorporate auxiliary information such as annotation uncertainty or a dependency among the labels. Building towards this, we propose a novel knowledge graph reformulation of multi-label classification, which not only readily increases predictive performance of an encoder but also serves as a general framework for introducing new domain knowledge.Specifically, we construct a multi-modal knowledge graph out of the chest X-ray images and its labels and pose multi-label classification as a link prediction problem. Incorporating auxiliary information can then simply be achieved by adding additional nodes and relations among them. When tested on a publicly-available radiograph dataset (CheXpert), our relational-reformulation using a naive knowledge graph outperforms the state-of-art by achieving an area-under-ROC curve of 83.5\%, an improvement of 1\% over a purely discriminative approach.},
	booktitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Sekuboyina, Anjany and O√±oro-Rubio, Daniel and Kleesiek, Jens and Malone, Brandon},
	month = apr,
	year = {2021},
	note = {ISSN: 1945-8452},
	keywords = {Annotations, Biomedical imaging, Radiography, Uncertainty, X-ray imaging, chest, knowledge graph, multi-label classification, radiographs, relational learning},
	pages = {1618--1622},
}

@inproceedings{wang_label_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {A {Label} {Embedding} {Method} for {Multi}-label {Classification} via {Exploiting} {Local} {Label} {Correlations}},
	isbn = {978-3-030-36802-9},
	doi = {10.1007/978-3-030-36802-9_19},
	abstract = {Multi-label learning has attracted more attention recently due to many real-world applications (e.g., text categorization and scene annotation). As the dimensionality of label space increases, it becomes more difficult to deal with this kind of applications. Therefore, dimensionality reduction techniques originally for feature space is also applied to label space, one of which is label embedding strategy which converts the high-dimensional label space into a low-dimensional reduced one. So far, existing label embedding methods mainly investigate the global recoverability between original labels and reduced labels, dependency between original features and reduced labels, or both. It is widely recognized that local label correlations could improve multi-label classification performance effectively. In this paper, we construct a trace ratio minimization problem as a novel label embedding criterion, which not only includes the global label recoverability and dependency, but also exploits the local label correlations as a local recoverability factor. Experiments on four benchmark data sets with more than 100 labels demonstrate that our proposed method is superior to four state-of-the-art techniques, according to two performance metrics for high-dimensional label space.},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Wang, Xidong and Li, Jun and Xu, Jianhua},
	editor = {Gedeon, Tom and Wong, Kok Wai and Lee, Minho},
	year = {2019},
	keywords = {Dimensionality reduction, Global recoverability, Label correlation, Local recoverability, Multi-label learning},
	pages = {168--180},
}

@article{hullermeier_flexible_2022,
	title = {A flexible class of dependence-aware multi-label loss functions},
	volume = {111},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-021-06107-2},
	doi = {10.1007/s10994-021-06107-2},
	abstract = {The idea to exploit label dependencies for better prediction is at the core of methods for multi-label classification (MLC), and performance improvements are normally explained in this way. Surprisingly, however, there is no established methodology that allows to analyze the dependence-awareness of MLC algorithms. With that goal in mind, we introduce a class of loss functions that are able to capture the important aspect of label dependence. To this end, we leverage the mathematical framework of non-additive measures and integrals. Roughly speaking, a non-additive measure allows for modeling the importance of correct predictions of label subsets (instead of single labels), and thereby their impact on the overall evaluation, in a flexible way. The well-known Hamming and subset 0/1 losses are rather extreme special cases of this function class, which give full importance to single label sets or the entire label set, respectively. We present concrete instantiations of this class, which appear to be especially appealing from a modeling perspective. The assessment of multi-label classifiers in terms of these losses is illustrated in an empirical study, clearly showing their aptness at capturing label dependencies. Finally, while not being the main goal of this study, we also show some preliminary results on the minimization of this parametrized family of losses.},
	language = {en},
	number = {2},
	urldate = {2022-06-23},
	journal = {Machine Learning},
	author = {H√ºllermeier, Eyke and Wever, Marcel and Loza Mencia, Eneldo and F√ºrnkranz, Johannes and Rapp, Michael},
	month = feb,
	year = {2022},
	keywords = {Analysis, Label dependence, Loss function, Multi-label classification, Non-additive measures},
	pages = {713--737},
}

@misc{puigcerver_scalable_2020,
	title = {Scalable {Transfer} {Learning} with {Expert} {Models}},
	url = {http://arxiv.org/abs/2009.13239},
	doi = {10.48550/arXiv.2009.13239},
	abstract = {Transfer of pre-trained representations can improve sample efficiency and reduce computational requirements for new tasks. However, representations used for transfer are usually generic, and are not tailored to a particular distribution of downstream tasks. We explore the use of expert representations for transfer with a simple, yet effective, strategy. We train a diverse set of experts by exploiting existing label structures, and use cheap-to-compute performance proxies to select the relevant expert for each target task. This strategy scales the process of transferring to new tasks, since it does not revisit the pre-training data during transfer. Accordingly, it requires little extra compute per target task, and results in a speed-up of 2-3 orders of magnitude compared to competing approaches. Further, we provide an adapter-based architecture able to compress many experts into a single model. We evaluate our approach on two different data sources and demonstrate that it outperforms baselines on over 20 diverse vision tasks in both cases.},
	urldate = {2022-06-23},
	publisher = {arXiv},
	author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Renggli, Cedric and Pinto, Andr√© Susano and Gelly, Sylvain and Keysers, Daniel and Houlsby, Neil},
	month = sep,
	year = {2020},
	note = {Number: arXiv:2009.13239
arXiv:2009.13239 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{erhan_why_nodate,
	title = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
	abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difÔ¨Åcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inÔ¨Çuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conÔ¨Årm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
	language = {en},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
	keywords = {‚õî No DOI found},
	pages = {36},
}

@inproceedings{zhang_imbalanced_2013,
	title = {Imbalanced multiple noisy labeling for supervised learning},
	booktitle = {Twenty-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Jing and Wu, Xindong and Sheng, Victor Shengli},
	year = {2013},
	keywords = {‚õî No DOI found, ‚õî No INSPIRE recid found},
}

@inproceedings{ma_gradient_2018,
	title = {Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ma, Yao and Olshevsky, Alexander and Szepesvari, Csaba and Saligrama, Venkatesh},
	year = {2018},
	keywords = {‚õî No DOI found},
	pages = {3335--3344},
}

@inproceedings{demartini_zencrowd_2012,
	title = {Zencrowd: leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking},
	shorttitle = {Zencrowd},
	doi = {10.1145/2187836.2187900},
	booktitle = {Proceedings of the 21st international conference on {World} {Wide} {Web}},
	author = {Demartini, Gianluca and Difallah, Djellel Eddine and Cudr√©-Mauroux, Philippe},
	year = {2012},
	pages = {469--478},
}

@inproceedings{ghosh_who_2011,
	title = {Who moderates the moderators? crowdsourcing abuse detection in user-generated content},
	shorttitle = {Who moderates the moderators?},
	doi = {10.1145/1993574.1993599},
	booktitle = {Proceedings of the 12th {ACM} conference on {Electronic} commerce},
	author = {Ghosh, Arpita and Kale, Satyen and McAfee, Preston},
	year = {2011},
	pages = {167--176},
}

@article{liu_variational_2012,
	title = {Variational {Inference} for {Crowdsourcing}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/cd00692c3bfe59267d5ecfac5310286c-Abstract.html},
	urldate = {2021-11-02},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Qiang and Peng, Jian and Ihler, Alexander T},
	year = {2012},
	keywords = {‚õî No DOI found},
}

@article{jiang_wrapper_2019,
	title = {Wrapper framework for test-cost-sensitive feature selection},
	volume = {51},
	number = {3},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Jiang, Liangxiao and Kong, Ganggang and Li, Chaoqun},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Classification accuracy, Data mining, Feature extraction, Geology, Medical diagnosis, Optimization, Support vector machines, Training, decision making, feature selection, test cost, test-cost-sensitive learning, ‚õî No DOI found},
	pages = {1747--1756},
}

@inproceedings{whitehill_whose_2009,
	title = {Whose vote should count more: {Optimal} integration of labels from labelers of unknown expertise},
	volume = {22},
	shorttitle = {Whose vote should count more},
	booktitle = {Advances in neural information processing systems},
	author = {Whitehill, Jacob and Wu, Ting-fan and Bergsma, Jacob and Movellan, Javier and Ruvolo, Paul},
	year = {2009},
	keywords = {‚õî No DOI found},
}

@inproceedings{lu_regularizing_2017,
	title = {Regularizing the loss layer of {CNNs} for facial expression recognition using crowdsourced labels},
	doi = {10.1109/IESYS.2017.8233557},
	booktitle = {2017 21st {Asia} {Pacific} {Symposium} on {Intelligent} and {Evolutionary} {Systems} ({IES})},
	publisher = {IEEE},
	author = {Lu, Philip and Li, Boyi and Shama, Saila and King, Irwin and Chan, Jonathan H.},
	year = {2017},
	pages = {31--36},
}

@article{welinder_multidimensional_2010,
	title = {The {Multidimensional} {Wisdom} of {Crowds}},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper/2010/hash/0f9cafd014db7a619ddb4276af0d692c-Abstract.html},
	urldate = {2021-11-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Welinder, Peter and Branson, Steve and Perona, Pietro and Belongie, Serge},
	year = {2010},
	keywords = {‚õî No DOI found},
}

@inproceedings{jia_newborn_2010,
	title = {Newborn footprint recognition using subspace learning methods},
	booktitle = {International {Conference} on {Intelligent} {Computing}},
	publisher = {Springer},
	author = {Jia, Wei and Gui, Jie and Hu, Rong-Xiang and Lei, Ying-Ke and Xiao, Xue-Yang},
	year = {2010},
	keywords = {‚õî No DOI found},
	pages = {447--453},
}

@inproceedings{raykar_supervised_2009,
	title = {Supervised learning from multiple experts: whom to trust when everyone lies a bit},
	shorttitle = {Supervised learning from multiple experts},
	doi = {10.1145/1553374.1553488},
	booktitle = {Proceedings of the 26th {Annual} international conference on machine learning},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Jerebko, Anna and Florin, Charles and Valadez, Gerardo Hermosillo and Bogoni, Luca and Moy, Linda},
	year = {2009},
	pages = {889--896},
}

@article{zheng_truth_2017,
	title = {Truth inference in crowdsourcing: {Is} the problem solved?},
	volume = {10},
	shorttitle = {Truth inference in crowdsourcing},
	doi = {10.14778/3055540.3055547},
	number = {5},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
	year = {2017},
	note = {Publisher: VLDB Endowment},
	pages = {541--552},
}

@inproceedings{abad_self-crowdsourcing_2017,
	title = {Self-crowdsourcing training for relation extraction},
	doi = {10.18653/v1/P17-2082},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	author = {Abad, Azad and Nabi, Moin and Moschitti, Alessandro},
	year = {2017},
	pages = {518--523},
}

@article{zhang_spectral_2016,
	title = {Spectral methods meet {EM}: {A} provably optimal algorithm for crowdsourcing},
	volume = {17},
	shorttitle = {Spectral methods meet {EM}},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},
	year = {2016},
	note = {Publisher: JMLR. org},
	keywords = {‚õî No DOI found},
	pages = {3537--3580},
}

@inproceedings{ayhan_test-time_2018,
	title = {Test-time {Data} {Augmentation} for {Estimation} of {Heteroscedastic} {Aleatoric} {Uncertainty} in {Deep} {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/Test-time-Data-Augmentation-for-Estimation-of-in-Ayhan-Berens/172df6d55b81f184ab0042c49634ccf9b72ed253},
	abstract = {This work proposes a simple but effective method using traditional data augmentation methods such as geometric and color transformations at test time to examine how much the network output varies in the vicinity of examples in the input spaces. Deep neural networks (DNNs) have revolutionized medical image analysis and disease diagnosis. Despite their impressive increase in performance, it is difficult to generate well-calibrated probabilistic outputs for such networks such that state-of-the-art networks fail to provide reliable uncertainty estimates regarding their decisions. We propose a simple but effective method using traditional data augmentation methods such as geometric and color transformations at test time. This allows to examine how much the network output varies in the vicinity of examples in the input spaces. Despite its simplicity, our method yields useful estimates for the input-dependent predictive uncertainties of deep neural networks. We showcase the impact of our method via the well-known collection of fundus images obtained from a previous Kaggle competition.},
	language = {en},
	urldate = {2022-04-25},
	author = {Ayhan, M. and Berens, Philipp},
	year = {2018},
	keywords = {‚õî No DOI found},
}

@inproceedings{li_resolving_2014,
	title = {Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation},
	doi = {10.1145/2588555.2610509},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} international conference on {Management} of data},
	author = {Li, Qi and Li, Yaliang and Gao, Jing and Zhao, Bo and Fan, Wei and Han, Jiawei},
	year = {2014},
	pages = {1187--1198},
}

@inproceedings{jin_leveraging_2018,
	title = {Leveraging label category relationships in multi-class crowdsourcing},
	doi = {10.1007/978-3-319-93037-4_11},
	booktitle = {Pacific-{Asia} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer},
	author = {Jin, Yuan and Du, Lan and Zhu, Ye and Carman, Mark},
	year = {2018},
	pages = {128--140},
}

@article{raykar_learning_2010,
	title = {Learning from crowds},
	volume = {11},
	issn = {15324435},
	abstract = {For many supervised learning tasks it may be infeasible (or very expensive) to obtain objective and reliable labels. Instead, we can collect subjective (possibly noisy) labels from multiple experts or annotators. In practice, there is a substantial amount of disagreement among the annotators, and hence it is of great practical interest to address conventional supervised learning problems in this scenario. In this paper we describe a probabilistic approach for supervised learning when we have multiple annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline. ¬© 2010 Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo H. Valadez, Charles Florin, Luca Bogoni and Linda Moy.},
	number = {43},
	journal = {Journal of Machine Learning Research},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Valadez, Gerardo Hermosillo and Florin, Charles and Bogoni, Luca and Moy, Linda and Raykar, C. and Valadez, Gerardo Hermosillo and Florin, Charles and Bogoni, Luca and Moy, Linda},
	year = {2010},
	keywords = {Crowdsourcing, Multiple annotators, Multiple experts, Multiple teachers, crowdsourcing, multiple annotators, multiple experts, multiple teachers, ‚õî No DOI found},
	pages = {1297--1322},
}

@inproceedings{kamar_identifying_2015,
	title = {Identifying and accounting for task-dependent bias in crowdsourcing},
	booktitle = {Third {AAAI} {Conference} on {Human} {Computation} and {Crowdsourcing}},
	author = {Kamar, Ece and Kapoor, Ashish and Horvitz, Eric},
	year = {2015},
	keywords = {‚õî No DOI found},
}

@inproceedings{khosla_novel_2011,
	title = {Novel dataset for fine-grained image categorization: {Stanford} dogs},
	volume = {2},
	shorttitle = {Novel dataset for fine-grained image categorization},
	booktitle = {Proc. {CVPR} {Workshop} on {Fine}-{Grained} {Visual} {Categorization} ({FGVC})},
	publisher = {Citeseer},
	author = {Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
	year = {2011},
	note = {Issue: 1},
	keywords = {‚õî No DOI found},
}

@article{bohanec_dex_1990,
	title = {{DEX}: {An} expert system shell for decision support},
	volume = {1},
	shorttitle = {{DEX}},
	number = {1},
	journal = {Sistemica},
	author = {Bohanec, Marko and Rajkoviƒç, Vladislav},
	year = {1990},
	note = {Publisher: Citeseer},
	keywords = {‚õî No DOI found},
	pages = {145--157},
}

@inproceedings{liu_improving_2017,
	title = {Improving {Learning}-from-{Crowds} through {Expert} {Validation}.},
	booktitle = {{IJCAI}},
	author = {Liu, Mengchen and Jiang, Liu and Liu, Junlin and Wang, Xiting and Zhu, Jun and Liu, Shixia},
	year = {2017},
	keywords = {‚õî No DOI found},
	pages = {2329--2336},
}

@article{zhang_class-specific_2020,
	title = {Class-specific attribute value weighting for {Naive} {Bayes}},
	volume = {508},
	journal = {Information Sciences},
	author = {Zhang, Huan and Jiang, Liangxiao and Yu, Liangjun},
	year = {2020},
	note = {Publisher: Elsevier},
	keywords = {‚ùì Multiple DOI},
	pages = {260--274},
}

@inproceedings{li_crowdsourced_2017,
	title = {Crowdsourced data management: {Overview} and challenges},
	shorttitle = {Crowdsourced data management},
	doi = {10.1145/3035918.3054776},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	author = {Li, Guoliang and Zheng, Yudian and Fan, Ju and Wang, Jiannan and Cheng, Reynold},
	year = {2017},
	pages = {1711--1716},
}

@inproceedings{zhang_differential_2018,
	title = {Differential evolution-based weighted majority voting for crowdsourcing},
	booktitle = {Pacific {Rim} international conference on artificial intelligence},
	publisher = {Springer},
	author = {Zhang, Hao and Jiang, Liangxiao and Xu, Wenqiang},
	year = {2018},
	keywords = {‚õî No DOI found},
	pages = {228--236},
}

@inproceedings{zhou_learning_2012,
	title = {Learning from the wisdom of crowds by minimax entropy,},
	booktitle = {Proc. {NIPS}},
	author = {Zhou, D. and Basu, S. and Mao, Y. and Platt, J.C.},
	year = {2012},
	keywords = {‚õî No DOI found},
	pages = {2195--2203},
}

@article{li_error_2014,
	title = {Error {Rate} {Bounds} and {Iterative} {Weighted} {Majority} {Voting} for {Crowdsourcing}},
	url = {http://arxiv.org/abs/1411.4086},
	abstract = {Crowdsourcing has become an effective and popular tool for human-powered computation to label large datasets. Since the workers can be unreliable, it is common in crowdsourcing to assign multiple workers to one task, and to aggregate the labels in order to obtain results of high quality. In this paper, we provide finite-sample exponential bounds on the error rate (in probability and in expectation) of general aggregation rules under the Dawid-Skene crowdsourcing model. The bounds are derived for multi-class labeling, and can be used to analyze many aggregation methods, including majority voting, weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We show that the oracle MAP rule approximately optimizes our upper bound on the mean error rate of weighted majority voting in certain setting. We propose an iterative weighted majority voting (IWMV) method that optimizes the error rate bound and approximates the oracle MAP rule. Its one step version has a provable theoretical guarantee on the error rate. The IWMV method is intuitive and computationally simple. Experimental results on simulated and real data show that IWMV performs at least on par with the state-of-the-art methods, and it has a much lower computational cost (around one hundred times faster) than the state-of-the-art methods.},
	urldate = {2021-11-16},
	journal = {arXiv:1411.4086 [cs, math, stat]},
	author = {Li, Hongwei and Yu, Bin},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4086},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning, ‚õî No DOI found},
}

@inproceedings{ma_adversarial_2020,
	title = {Adversarial crowdsourcing through robust rank-one matrix completion},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ma, Qianqian and Olshevsky, Alex},
	year = {2020},
	keywords = {Computer Science - Machine Learning, ‚õî No DOI found},
	pages = {21841--21852},
}

@inproceedings{dalvi_aggregating_2013,
	title = {Aggregating crowdsourced binary ratings},
	doi = {10.1145/2488388.2488414},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	author = {Dalvi, Nilesh and Dasgupta, Anirban and Kumar, Ravi and Rastogi, Vibhor},
	year = {2013},
	pages = {285--294},
}

@article{zhang_ceka_2015,
	title = {{CEKA}: a tool for mining the wisdom of crowds.},
	volume = {16},
	shorttitle = {{CEKA}},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Zhang, Jing and Sheng, Victor S. and Nicholson, Bryce and Wu, Xindong},
	year = {2015},
	keywords = {‚õî No DOI found},
	pages = {2853--2858},
}

@inproceedings{russakovsky_best_2015,
	title = {Best of both worlds: human-machine collaboration for object annotation},
	shorttitle = {Best of both worlds},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Russakovsky, Olga and Li, Li-Jia and Fei-Fei, Li},
	year = {2015},
	keywords = {‚õî No DOI found},
	pages = {2121--2131},
}

@article{frosst_distilling_2017,
	title = {Distilling a {Neural} {Network} {Into} a {Soft} {Decision} {Tree}},
	url = {http://arxiv.org/abs/1711.09784},
	abstract = {Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
	urldate = {2021-11-16},
	journal = {arXiv:1711.09784 [cs, stat]},
	author = {Frosst, Nicholas and Hinton, Geoffrey},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, ‚õî No DOI found, ‚õî No INSPIRE recid found},
}

@inproceedings{ustalov_general-purpose_2021,
	title = {A {General}-{Purpose} {Crowdsourcing} {Computational} {Quality} {Control} {Toolkit} for {Python}},
	booktitle = {{arXiv} preprint {arXiv}:2109.08584},
	author = {Ustalov, Dmitry and Pavlichenko, Nikita and Losev, Vladimir and Giliazev, Iulian and Tulin, Evgeny},
	year = {2021},
	keywords = {‚õî No DOI found},
}

@inproceedings{sheng_get_2008,
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	shorttitle = {Get another label?},
	doi = {10.1145/1401890.1401965},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	year = {2008},
	pages = {614--622},
}

@inproceedings{karger_efficient_2013,
	title = {Efficient crowdsourcing for multi-class labeling},
	doi = {10.1145/2465529.2465761},
	booktitle = {Proceedings of the {ACM} {SIGMETRICS}/international conference on {Measurement} and modeling of computer systems},
	author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
	year = {2013},
	pages = {81--92},
}

@misc{noauthor_csiro_nodate,
	title = {{CSIRO} {Data} {Access} {Portal} - {Synthetic} {Cerebral} {Microbleed} on {SWI} images},
	url = {https://data.csiro.au/collection/csiro:50304},
	urldate = {2022-06-20},
}

@inproceedings{hutchison_parallel_2009,
	address = {Berlin, Heidelberg},
	title = {Parallel {K}-{Means} {Clustering} {Based} on {MapReduce}},
	volume = {5931},
	isbn = {978-3-642-10664-4 978-3-642-10665-1},
	url = {http://link.springer.com/10.1007/978-3-642-10665-1_71},
	doi = {10.1007/978-3-642-10665-1_71},
	abstract = {Data clustering has been received considerable attention in many applications, such as data mining, document retrieval, image segmentation and pattern classiÔ¨Åcation. The enlarging volumes of information emerging by the progress of technology, makes clustering of very large scale of data a challenging task. In order to deal with the problem, many researchers try to design eÔ¨Écient parallel clustering algorithms. In this paper, we propose a parallel k -means clustering algorithm based on MapReduce, which is a simple yet powerful parallel programming technique. The experimental results demonstrate that the proposed algorithm can scale well and eÔ¨Éciently process large datasets on commodity hardware.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {Cloud {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zhao, Weizhong and Ma, Huifang and He, Qing},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Jaatun, Martin Gilje and Zhao, Gansen and Rong, Chunming},
	year = {2009},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Data mining, Hadoop, K-means, MapReduce, Parallel clustering},
	pages = {674--679},
}

@article{kurve_multicategory_2015,
	title = {Multicategory {Crowdsourcing} {Accounting} for {Variable} {Task} {Difficulty}, {Worker} {Skill}, and {Worker} {Intention}},
	volume = {27},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2014.2327026},
	abstract = {Crowdsourcing allows instant recruitment of workers on the web to annotate image, webpage, or document databases. However, worker unreliability prevents taking a worker's responses at ‚Äúface value‚Äù. Thus, responses from multiple workers are typically aggregated to more reliably infer ground-truth answers. We study two approaches for crowd aggregation on multicategory answer spaces: stochastic modeling-based and deterministic objective function-based. Our stochastic model for answer generation plausibly captures the interplay between worker skills, intentions, and task difficulties and captures a broad range of worker types. Our deterministic objective-based approach aims to maximize the average aggregate confidence of weighted plurality crowd decision making. In both approaches, we explicitly model the skill and intention of individual workers, which is exploited for improved crowd aggregation. Our methods are applicable in both unsupervised and semi-supervised settings, and also when the batch of tasks is heterogeneous, i.e., from multiple domains, with task-dependent answer spaces. As observed experimentally, the proposed methods can defeat ‚Äútyranny of the masses‚Äù, i.e., they are especially advantageous when there is an (a priori unknown) minority of skilled workers amongst a large crowd of unskilled (and malicious) workers.},
	number = {3},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Kurve, Aditya and Miller, David J. and Kesidis, George},
	month = mar,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Accuracy, Aggregates, Computational modeling, Crowdsourcing, Data models, Probes, Stochastic processes, ensemble classification, expectation-maximization, inference, multicategory},
	pages = {794--809},
}

@inproceedings{sheshadri_square_2013,
	title = {{SQUARE}: {A} {Benchmark} for {Research} on {Computing} {Crowd} {Consensus}},
	volume = {1},
	copyright = {Copyright (c) 2013 Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	shorttitle = {{SQUARE}},
	url = {https://ojs.aaai.org/index.php/HCOMP/article/view/13088},
	abstract = {While many statistical consensus methods now exist, relatively little comparative benchmarking and integration of techniques has made it increasingly difficult to determine the current state-of-the-art, to evaluate the relative benefit of new methods, to understand where specific problems merit greater attention, and to measure field progress over time. To make such comparative evaluation easier for everyone, we present SQUARE, an open source shared task framework including benchmark datasets, defined tasks, standard metrics, and reference implementations with empirical results for several popular methods. In addition to measuring performance on a variety of public, real crowd datasets, the benchmark also varies supervision and noise by manipulating training size and labeling error. We envision SQUARE as dynamic and continually evolving, with new datasets and reference implementations being added according to community needs and interest. We invite community contributions and participation.},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Human} {Computation} and {Crowdsourcing}},
	author = {Sheshadri, Aashish and Lease, Matthew},
	month = nov,
	year = {2013},
	keywords = {Aggregation, Benchmarking, Consensus, Crowdsourcing, Human Computation},
	pages = {156--164},
}

@article{hernandez-gonzalez_note_2019,
	title = {A {Note} on the {Behavior} of {Majority} {Voting} in {Multi}-{Class} {Domains} with {Biased} {Annotators}},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2845400},
	abstract = {Majority voting is a popular and robust strategy to aggregate different opinions in learning from crowds, where each worker labels examples according to their own criteria. Although it has been extensively studied in the binary case, its behavior with multiple classes is not completely clear, specifically when annotations are biased. This paper attempts to fill that gap. The behavior of the majority voting strategy is studied in-depth in multi-class domains, emphasizing the effect of annotation bias. By means of a complete experimental setting, we show the limitations of the standard majority voting strategy. The use of three simple techniques that infer global information from the annotations and annotators allows us to put the performance of the majority voting strategy in context.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Hern√°ndez-Gonz√°lez, Jer√≥nimo and Inza, I√±aki and Lozano, Jose A.},
	month = jan,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Aggregates, Labeling, Multi-class learning, Noise measurement, Robustness, Standards, Training, biased annotations, learning from crowds},
	pages = {195--200},
}

@article{zhang_crowdsourced_2019,
	title = {Crowdsourced {Label} {Aggregation} {Using} {Bilayer} {Collaborative} {Clustering}},
	volume = {30},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2018.2890148},
	abstract = {With online crowdsourcing platforms, labels can be acquired at relatively low costs from massive nonexpert workers. To improve the quality of labels obtained from these imperfect crowdsourced workers, we usually let different workers provide labels for the same instance. Then, the true labels for all instances are estimated from these multiple noisy labels. This traditional general-purpose label aggregation process, solely relying on the collected noisy labels, cannot significantly improve the accuracy of integrated labels under a low labeling quality circumstance. This paper proposes a novel bilayer collaborative clustering (BLCC) method for the label aggregation in crowdsourcing. BLCC first generates the conceptual-level features for the instances from their multiple noisy labels and infers the initially integrated labels by performing clustering on the conceptual-level features. Then, it performs another clustering on the physical-level features to form the estimations of the true labels on the physical layer. The clustering results on both layers can facilitate in tracking the changes in the uncertainties of the instances. Finally, the initially integrated labels that are likely to be wrongly inferred on the conceptual layer can be addressed using the estimated labels on the physical layer. The clustering processes on both layers can keep providing guidance information for each other in the multiple label remedy rounds. The experimental results on 12 real-world crowdsourcing data sets show that the performance of the proposed method in terms of accuracy is better than that of the state-of-the-art methods.},
	number = {10},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Jing and Sheng, Victor S. and Wu, Jian},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Clustering, Collaboration, Crowdsourcing, Feature extraction, Labeling, Noise measurement, Probabilistic logic, Reliability, crowdsourcing, label aggregation, label noise handling, truth inference},
	pages = {3172--3185},
}

@article{sheng_majority_2019,
	title = {Majority {Voting} and {Pairing} with {Multiple} {Noisy} {Labeling}},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2017.2659740},
	abstract = {With the crowdsourcing of small tasks becoming easier, it is possible to obtain non-expert/imperfect labels at low cost. With low-cost imperfect labeling, it is straightforward to collect multiple labels for the same data items. This paper proposes strategies of utilizing these multiple labels for supervised learning, based on two basic ideas: majority voting and pairing. We show several interesting results based on our experiments. (i) The strategies based on the majority voting idea work well under the situation where the certainty level is high. (ii) On the contrary, the pairing strategies are more preferable under the situation where the certainty level is low. (iii) Among the majority voting strategies, soft majority voting can reduce the bias and roughness, and perform better than majority voting. (iv) Pairing can completely avoid the bias by having both sides (potentially correct and incorrect/noisy information) considered. Beta estimation is applied to reduce the impact of the noise in pairing. Our experimental results show that pairing with Beta estimation always performs well under different certainty levels. (v) All strategies investigated are labeling quality agnostic strategies for real-world applications, and some of them perform better than or at least very close to the gnostic strategies.},
	number = {7},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Sheng, Victor S. and Zhang, Jing and Gu, Bin and Wu, Xindong},
	month = jul,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Crowdsourcing, Data models, Labeling, Logistics, Noise measurement, Training, Training data, classification, data preprocessing, multiple noisy labels},
	pages = {1355--1368},
}

@article{zhang_improving_2018,
	title = {Improving {Crowdsourced} {Label} {Quality} {Using} {Noise} {Correction}},
	volume = {29},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2017.2677468},
	abstract = {Crowdsourcing systems provide a cost effective and convenient way to collect labels, but they often fail to guarantee the quality of the labels. This paper proposes a novel framework that introduces noise correction techniques to further improve the quality of integrated labels that are inferred from the multiple noisy labels of objects. In the proposed general framework, information about the qualities of labelers estimated by a front-end ground truth inference algorithm is utilized to supervise subsequent label noise filtering and correction. The framework uses a novel algorithm termed adaptive voting noise correction (AVNC) to precisely identify and correct the potential noisy labels. After filtering out the instances with noisy labels, the remaining cleansed data set is used to create multiple weak classifiers, based on which a powerful ensemble classifier is induced to correct these noises. Experimental results on eight simulated data sets with different kinds of features and two real-world crowdsourcing data sets in different domains consistently show that: 1) the proposed framework can improve label quality regardless of inference algorithms, especially under the circumstance that each instance has a few repeated labels and 2) since the proposed AVNC algorithm considers both the number of and the probability of potential label noises, it outperforms the state-of-the-art noise correction algorithms.},
	number = {5},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Jing and Sheng, Victor S. and Li, Tao and Wu, Xindong},
	month = may,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Algorithm design and analysis, Crowdsourcing, Inference algorithms, Labeling, Noise measurement, Prediction algorithms, Predictive models, ground truth inference, label integration, label noise correction, label quality},
	pages = {1675--1688},
}

@article{gui_supervised_2018,
	title = {Supervised {Discrete} {Hashing} {With} {Relaxation}},
	volume = {29},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2016.2636870},
	abstract = {Data-dependent hashing has recently attracted attention due to being able to support efficient retrieval and storage of high-dimensional data, such as documents, images, and videos. In this paper, we propose a novel learning-based hashing method called ‚Äúsupervised discrete hashing with relaxation‚Äù (SDHR) based on ‚Äúsupervised discrete hashing‚Äù (SDH). SDH uses ordinary least squares regression and traditional zero-one matrix encoding of class label information as the regression target (code words), thus fixing the regression target. In SDHR, the regression target is instead optimized. The optimized regression target matrix satisfies a large margin constraint for correct classification of each example. Compared with SDH, which uses the traditional zero-one matrix, SDHR utilizes the learned regression target matrix and, therefore, more accurately measures the classification error of the regression model and is more flexible. As expected, SDHR generally outperforms SDH. Experimental results on two large-scale image data sets (CIFAR-10 and MNIST) and a large-scale and challenging face data set (FRGC) demonstrate the effectiveness and efficiency of SDHR.},
	number = {3},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Gui, Jie and Liu, Tongliang and Sun, Zhenan and Tao, Dacheng and Tan, Tieniu},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Data-dependent hashing, Kernel, Learning systems, Linear programming, Optimization, Quantization (signal), Synchronous digital hierarchy, Training, least squares regression, supervised discrete hashing (SDH), supervised discrete hashing with relaxation (SDHR)},
	pages = {608--617},
}

@article{muhammadi_unified_2015,
	title = {A unified statistical framework for crowd labeling},
	volume = {45},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-014-0790-7},
	doi = {10.1007/s10115-014-0790-7},
	abstract = {Recently, there has been a burst in the number of research projects on human computation via crowdsourcing. Multiple-choice (or labeling) questions could be referred to as a common type of problem which is solved by this approach. As an application, crowd labeling is applied to find true labels for large machine learning datasets. Since crowds are not necessarily experts, the labels they provide are rather noisy and erroneous. This challenge is usually resolved by collecting multiple labels for each sample and then aggregating them to estimate the true label. Although the mechanism leads to high-quality labels, it is not actually cost-effective. As a result, efforts are currently made to maximize the accuracy in estimating true labels, while fixing the number of acquired labels.},
	language = {en},
	number = {2},
	urldate = {2021-11-16},
	journal = {Knowledge and Information Systems},
	author = {Muhammadi, Jafar and Rabiee, Hamid R. and Hosseini, Abbas},
	month = nov,
	year = {2015},
	pages = {271--294},
}

@inproceedings{kunde_recommending_2020,
	address = {New York, NY, USA},
	series = {{RecSys} '20},
	title = {Recommending in changing times},
	isbn = {978-1-4503-7583-2},
	url = {https://doi.org/10.1145/3383313.3418492},
	doi = {10.1145/3383313.3418492},
	abstract = {Recommender systems today face major challenges in keeping up with dynamic customer preferences. Disruptions or sudden changes in the environment affect customer preferences drastically and render historical data ineffective for modeling. With businesses relying heavily on Machine Learning(ML) based recommender systems for catering to customer preferences, the accuracy of timely recommendations gains prime significance. To address these challenges, we propose a novel concept, LDT (Labeled Data Threshold), a newly defined parameter to determine the sufficiency of available labeled training data. Our proposed scheme, using LDT leads to a significant reduction ( 50X) in the training time for a model, thus enabling recommender systems to adapt quickly to disruptions. We illustrate the efficacy of our proposed scheme, by conducting extensive experimental analysis on six well known, structured data sets from various public domains.},
	urldate = {2021-11-16},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kunde, Shruti and Mishra, Mayank and Pandit, Amey and Singhal, Rekha and Nambiar, Manoj Karunakaran and Shroff, Gautam and Gupta, Shashank},
	month = sep,
	year = {2020},
	keywords = {Automated Data Labeling, Labeled Data Threshold, Machine Learning, Supervised Learning},
	pages = {714--719},
}

@article{zhang_multi-class_2016,
	title = {Multi-{Class} {Ground} {Truth} {Inference} in {Crowdsourcing} with {Clustering}},
	volume = {28},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2015.2504974},
	abstract = {Due to low quality of crowdsourced labelers, the integrated label of each example is usually inferred from its multiple noisy labels provided by different labelers. This paper proposes a novel algorithm, Ground Truth Inference using Clustering (GTIC), to improve the quality of integrated labels for multi-class labeling. For a K labeling case, GTIC utilizes the multiple noisy label sets of examples to generate features. Then, it uses a K-Means algorithm to cluster all examples into K different groups, each of which is mapped to a specific class. Examples in the same cluster are assigned a corresponding class label. We compare GTIC with four existing multi-class ground truth inference algorithms, majority voting (MV), Dawid \& Skene's (DS), ZenCrowd (ZC) and Spectral DS (SDS), on one synthetic and eight real-world datasets. Experimental results show that the performance of GTIC is significantly superior to the others in terms of both accuracy and M-AUC. Besides, the running time of GTIC is about twenty times faster than EM-based complicated inference algorithms.},
	number = {4},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Jing and Sheng, Victor S. and Wu, Jian and Wu, Xindong},
	month = apr,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Clustering, Clustering algorithms, Computer science, Crowdsourcing, EM algorithm, Inference algorithms, Labeling, Maximum likelihood estimation, Noise measurement, clustering,, ground truth inference, multi-class labeling},
	pages = {1080--1085},
}

@article{zhu_constrained_2016,
	title = {Constrained {Clustering} {With} {Imperfect} {Oracles}},
	volume = {27},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/7018990/},
	doi = {10.1109/TNNLS.2014.2387425},
	abstract = {While clustering is usually an unsupervised operation, there are circumstances where we have access to prior belief that pairs of samples should (or should not) be assigned with the same cluster. Constrained clustering aims to exploit this prior belief as constraint (or weak supervision) to inÔ¨Çuence the cluster formation so as to obtain a data structure more closely resembling human perception. Two important issues remain open: 1) how to exploit sparse constraints effectively and 2) how to handle ill-conditioned/noisy constraints generated by imperfect oracles. In this paper, we present a novel pairwise similarity measure framework to address the above issues. SpeciÔ¨Åcally, in contrast to existing constrained clustering approaches that blindly rely on all features for constraint propagation, our approach searches for neighborhoods driven by discriminative feature selection for more effective constraint diffusion. Crucially, we formulate a novel approach to handling the noisy constraint problem, which has been unrealistically ignored in the constrained clustering literature. Extensive comparative results show that our method is superior to the state-of-the-art constrained clustering approaches and can generally beneÔ¨Åt existing pairwise similarity-based data clustering algorithms, such as spectral clustering and afÔ¨Ånity propagation.},
	language = {en},
	number = {6},
	urldate = {2021-11-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhu, Xiatian and Loy, Chen Change and Gong, Shaogang},
	month = jun,
	year = {2016},
	pages = {1345--1357},
}

@article{tao_domain-weighted_2019,
	title = {Domain-{Weighted} {Majority} {Voting} for {Crowdsourcing}},
	volume = {30},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2018.2836969},
	abstract = {Crowdsourcing labeling systems provide an efficient way to generate multiple inaccurate labels for given observations. If the competence level or the ‚Äúreputation,‚Äù which can be explained as the probabilities of annotating the right label, for each crowdsourcing annotators is equal and biased to annotate the right label, majority voting (MV) is the optimal decision rule for merging the multiple labels into a single reliable one. However, in practice, the competence levels of annotators employed by the crowdsourcing labeling systems are often diverse very much. In these cases, weighted MV is more preferred. The weights should be determined by the competence levels. However, since the annotators are anonymous and the ground-truth labels are usually unknown, it is hard to compute the competence levels of the annotators directly. In this paper, we propose to learn the weights for weighted MV by exploiting the expertise of annotators. Specifically, we model the domain knowledge of different annotators with different distributions and treat the crowdsourcing problem as a domain adaptation problem. The annotators provide labels to the source domains and the target domain is assumed to be associated with the ground-truth labels. The weights are obtained by matching the source domains with the target domain. Although the target-domain labels are unknown, we prove that they could be estimated under mild conditions. Both theoretical and empirical analyses verify the effectiveness of the proposed method. Large performance gains are shown for specific data sets.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tao, Dapeng and Cheng, Jun and Yu, Zhengtao and Yue, Kun and Wang, Lizhen},
	month = jan,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Crowdsourcing, Labeling, Machine learning, Noise measurement, Reliability, Task analysis, Training, domain knowledge, theoretical guarantee, weighted majority voting (MV)},
	pages = {163--174},
}

@article{polap_multi-threaded_2018,
	title = {Multi-threaded learning control mechanism for neural networks},
	volume = {87},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18300931},
	doi = {10.1016/j.future.2018.04.050},
	abstract = {Neural networks are applicable in many solutions for classification, prediction, control, etc. The variety of purposes is growing but with each new application the expectations are higher. We want neural networks to be more precise independently of the input data. Efficiency of the processing in a large manner depends on the training algorithm. Basically this procedure is based on the random selection of weights in which neurons connections are burdened. During training process we implement a method which involves modification of the weights to minimize the response error of the entire structure. Training continues until the minimum error value is reached ‚Äî¬†however in general the smaller it is, the time of weight modification is longer. Another problem is that training with the same set of data can cause different training times depending on the initial weight selection. To overcome arising problems we need a method that will boost the procedure and support final precision. In this article, we propose the use of multi-threading mechanism to minimize training time by rejecting unnecessary weights selection. In the mechanism we use a multi-core solution to select the best weights between all parallel trained networks. Proposed solution was tested for three types of neural networks (classic, sparking and convolutional) using sample classification problems. The results have shown positive aspects of the proposed idea: shorter training time and better efficiency in various tasks.},
	language = {en},
	urldate = {2021-11-16},
	journal = {Future Generation Computer Systems},
	author = {Po≈Çap, Dawid and Wo≈∫niak, Marcin and Wei, Wei and Dama≈°eviƒçius, Robertas},
	month = oct,
	year = {2018},
	keywords = {Back-propagation algorithm, Multi-threading, Neural networks},
	pages = {16--34},
}

@article{allahbakhsh_quality_2013,
	title = {Quality {Control} in {Crowdsourcing} {Systems}: {Issues} and {Directions}},
	volume = {17},
	shorttitle = {Quality {Control} in {Crowdsourcing} {Systems}},
	doi = {10.1109/MIC.2013.20},
	abstract = {As a new distributed computing model, crowdsourcing lets people leverage the crowd's intelligence and wisdom toward solving problems. This article proposes a framework for characterizing various dimensions of quality control in crowdsourcing systems, a critical issue. The authors briefly review existing quality-control approaches, identify open issues, and look to future research directions. In the Web extra, the authors discuss both design-time and runtime approaches in more detail.},
	journal = {Internet Computing, IEEE},
	author = {Allahbakhsh, Mohammad and Benatallah, Boualem and Ignjatoviƒá, Aleksandar and Motahari Nezhad, Hamid R. and Bertino, Elisa and Dustdar, Schahram},
	month = mar,
	year = {2013},
	pages = {76--81},
}

@article{cao_binary_2018,
	title = {Binary {Hashing} for {Approximate} {Nearest} {Neighbor} {Search} on {Big} {Data}: {A} {Survey}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Binary {Hashing} for {Approximate} {Nearest} {Neighbor} {Search} on {Big} {Data}},
	doi = {10.1109/ACCESS.2017.2781360},
	abstract = {Nearest neighbor search is a fundamental problem in various domains, such as computer vision, data mining, and machine learning. With the explosive growth of data on the Internet, many new data structures using spatial partitions and recursive hyperplane decomposition (e.g., k-d trees) are proposed to speed up the nearest neighbor search. However, these data structures are facing big data challenges. To meet these challenges, binary hashing-based approximate nearest neighbor search methods attract substantial attention due to their fast query speed and drastically reduced storage. Since the most notably locality sensitive hashing was proposed, a large number of binary hashing methods have emerged. In this paper, we first illustrate the development of binary hashing research by proposing an overall and clear classification of them. Then we conduct extensive experiments to compare the performance of these methods on five famous and public data sets. Finally, we present our view on this topic.},
	journal = {IEEE Access},
	author = {Cao, Yuan and Qi, Heng and Zhou, Wenrui and Kato, Jien and Li, Keqiu and Liu, Xiulong and Gui, Jie},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Approximate nearest neighbor search, Binary codes, Data mining, Distributed databases, Encoding, Hamming distance, Nearest neighbor searches, hashing based methods, large-scale database, overview},
	pages = {2039--2054},
}

@article{varon_noise_2015,
	title = {Noise {Level} {Estimation} for {Model} {Selection} in {Kernel} {PCA} {Denoising}},
	volume = {26},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2015.2388696},
	abstract = {One of the main challenges in unsupervised learning is to find suitable values for the model parameters. In kernel principal component analysis (kPCA), for example, these are the number of components, the kernel, and its parameters. This paper presents a model selection criterion based on distance distributions (MDDs). This criterion can be used to find the number of components and the œÉ2 parameter of radial basis function kernels by means of spectral comparison between information and noise. The noise content is estimated from the statistical moments of the distribution of distances in the original dataset. This allows for a type of randomization of the dataset, without actually having to permute the data points or generate artificial datasets. After comparing the eigenvalues computed from the estimated noise with the ones from the input dataset, information is retained and maximized by a set of model parameters. In addition to the model selection criterion, this paper proposes a modification to the fixed-size method and uses the incomplete Cholesky factorization, both of which are used to solve kPCA in large-scale applications. These two approaches, together with the model selection MDD, were tested in toy examples and real life applications, and it is shown that they outperform other known algorithms.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Varon, Carolina and Alzate, Carlos and Suykens, Johan A. K.},
	month = nov,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Eigenvalues and eigenfunctions, Estimation, Kernel, Kernel principal component analysis (kPCA), Noise, Noise level, Noise reduction, Principal component analysis, least squares support vector machines (LS-SVMs), noise level estimation, unsupervised learning, unsupervised learning.},
	pages = {2650--2663},
}

@article{han_progressive_2018,
	title = {Progressive {Stochastic} {Learning} for {Noisy} {Labels}},
	volume = {29},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2018.2792062},
	abstract = {Large-scale learning problems require a plethora of labels that can be efficiently collected from crowdsourcing services at low cost. However, labels annotated by crowdsourced workers are often noisy, which inevitably degrades the performance of large-scale optimizations including the prevalent stochastic gradient descent (SGD). Specifically, these noisy labels adversely affect updates of the primal variable in conventional SGD. To solve this challenge, we propose a robust SGD mechanism called progressive stochastic learning (POSTAL), which naturally integrates the learning regime of curriculum learning (CL) with the update process of vanilla SGD. Our inspiration comes from the progressive learning process of CL, namely learning from ‚Äúeasy‚Äù tasks to ‚Äúcomplex‚Äù tasks. Through the robust learning process of CL, POSTAL aims to yield robust updates of the primal variable on an ordered label sequence, namely, from ‚Äúreliable‚Äù labels to ‚Äúnoisy‚Äù labels. To realize POSTAL mechanism, we design a cluster of ‚Äúscreening losses,‚Äù which sorts all labels from the reliable region to the noisy region. To sum up, POSTAL using screening losses ensures robust updates of the primal variable on reliable labels first, then on noisy labels incrementally until convergence. In theory, we derive the convergence rate of POSTAL realized by screening losses. Meanwhile, we provide the robustness analysis of representative screening losses. Experimental results on UCI1 simulated and Amazon Mechanical Turk crowdsourcing data sets show that the POSTAL using screening losses is more effective and robust than several existing baselines.1UCI is the abbreviation of University of California Irvine.},
	number = {10},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Han, Bo and Tsang, Ivor W. and Chen, Ling and Yu, Celina P. and Fung, Sai-Fu},
	month = oct,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Convergence, Crowdsourcing, Noise measurement, Progressive stochastic learning (POSTAL), Robustness, Stochastic processes, Task analysis, curriculum learning (CL), effectiveness, noisy labels, robustness, screening losses, stochastic gradient descent (SGD)},
	pages = {5136--5148},
}

@article{miao_rboost_2016,
	title = {{RBoost}: {Label} {Noise}-{Robust} {Boosting} {Algorithm} {Based} on a {Nonconvex} {Loss} {Function} and the {Numerically} {Stable} {Base} {Learners}},
	volume = {27},
	issn = {2162-2388},
	shorttitle = {{RBoost}},
	doi = {10.1109/TNNLS.2015.2475750},
	abstract = {AdaBoost has attracted much attention in the machine learning community because of its excellent performance in combining weak classifiers into strong classifiers. However, AdaBoost tends to overfit to the noisy data in many applications. Accordingly, improving the antinoise ability of AdaBoost plays an important role in many applications. The sensitiveness to the noisy data of AdaBoost stems from the exponential loss function, which puts unrestricted penalties to the misclassified samples with very large margins. In this paper, we propose two boosting algorithms, referred to as RBoost1 and RBoost2, which are more robust to the noisy data compared with AdaBoost. RBoost1 and RBoost2 optimize a nonconvex loss function of the classification margin. Because the penalties to the misclassified samples are restricted to an amount less than one, RBoost1 and RBoost2 do not overfocus on the samples that are always misclassified by the previous base learners. Besides the loss function, at each boosting iteration, RBoost1 and RBoost2 use numerically stable ways to compute the base learners. These two improvements contribute to the robustness of the proposed algorithms to the noisy training and testing samples. Experimental results on the synthetic Gaussian data set, the UCI data sets, and a real malware behavior data set illustrate that the proposed RBoost1 and RBoost2 algorithms perform better when the training data sets contain noisy data.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Miao, Qiguang and Cao, Ying and Xia, Ge and Gong, Maoguo and Liu, Jiachen and Song, Jianfeng},
	month = nov,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {AdaBoost, Algorithm design and analysis, Boosting, Estimation, Noise, Noise measurement, Robustness, Training, adaptive Newton step, antinoise, loss function},
	pages = {2216--2228},
}

@article{qiao_sparsity_2010,
	title = {Sparsity preserving projections with applications to face recognition},
	volume = {43},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320309001964},
	doi = {10.1016/j.patcog.2009.05.005},
	abstract = {Dimensionality reduction methods (DRs) have commonly been used as a principled way to understand the high-dimensional data such as face images. In this paper, we propose a new unsupervised DR method called sparsity preserving projections (SPP). Unlike many existing techniques such as local preserving projection (LPP) and neighborhood preserving embedding (NPE), where local neighborhood information is preserved during the DR procedure, SPP aims to preserve the sparse reconstructive relationship of the data, which is achieved by minimizing a L1 regularization-related objective function. The obtained projections are invariant to rotations, rescalings and translations of the data, and more importantly, they contain natural discriminating information even if no class labels are provided. Moreover, SPP chooses its neighborhood automatically and hence can be more conveniently used in practice compared to LPP and NPE. The feasibility and effectiveness of the proposed method is verified on three popular face databases (Yale, AR and Extended Yale B) with promising results.},
	language = {en},
	number = {1},
	urldate = {2021-11-16},
	journal = {Pattern Recognition},
	author = {Qiao, Lishan and Chen, Songcan and Tan, Xiaoyang},
	month = jan,
	year = {2010},
	keywords = {Compressive sensing, Dimensionality reduction, Face recognition, Sparse representation},
	pages = {331--341},
}

@article{duan_learning_2017,
	title = {Learning {With} {Auxiliary} {Less}-{Noisy} {Labels}},
	volume = {28},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2016.2546956},
	abstract = {Obtaining a sufficient number of accurate labels to form a training set for learning a classifier can be difficult due to the limited access to reliable label resources. Instead, in real-world applications, less-accurate labels, such as labels from nonexpert labelers, are often used. However, learning with less-accurate labels can lead to serious performance deterioration because of the high noise rate. Although several learning methods (e.g., noise-tolerant classifiers) have been advanced to increase classification performance in the presence of label noise, only a few of them take the noise rate into account and utilize both noisy but easily accessible labels and less-noisy labels, a small amount of which can be obtained with an acceptable added time cost and expense. In this brief, we propose a learning method, in which not only noisy labels but also auxiliary less-noisy labels, which are available in a small portion of the training data, are taken into account. Based on a flipping probability noise model and a logistic regression classifier, this method estimates the noise rate parameters, infers ground-truth labels, and learns the classifier simultaneously in a maximum likelihood manner. The proposed method yields three learning algorithms, which correspond to three prior knowledge states regarding the less-noisy labels. The experiments show that the proposed method is tolerant to label noise, and outperforms classifiers that do not explicitly consider the auxiliary less-noisy labels.},
	number = {7},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Duan, Yunyan and Wu, Ou},
	month = jul,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Crowdsourcing, Learning systems, Maximum likelihood approach, Maximum likelihood estimation, Noise measurement, Probabilistic logic, Training, Training data, noisy degrees, noisy labels, soft constraints},
	pages = {1716--1721},
}

@article{lei_feature_2012,
	title = {Feature extraction using orthogonal discriminant local tangent space alignment},
	volume = {15},
	issn = {1433-755X},
	url = {https://doi.org/10.1007/s10044-011-0231-0},
	doi = {10.1007/s10044-011-0231-0},
	abstract = {A novel algorithm called orthogonal discriminant local tangent space alignment (O-DLTSA) is proposed for supervised feature extraction. Derived from local tangent space alignment (LTSA), O-DLTSA not only inherits the advantages of LTSA which uses local tangent space as a representation of the local geometry so as to preserve the local structure, but also makes full use of class information and orthogonal subspace to improve discriminant power. The experimental results of applying O-DLTSA to standard face databases demonstrate the effectiveness of the proposed method.},
	language = {en},
	number = {3},
	urldate = {2021-11-16},
	journal = {Pattern Analysis and Applications},
	author = {Lei, Ying-Ke and Xu, Yang-Ming and Yang, Jun-An and Ding, Zhi-Guo and Gui, Jie},
	month = aug,
	year = {2012},
	pages = {249--259},
}

@article{gui_fast_2018,
	title = {Fast {Supervised} {Discrete} {Hashing}},
	volume = {40},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2017.2678475},
	abstract = {Learning-based hashing algorithms are ‚Äúhot topics‚Äù because they can greatly increase the scale at which existing methods operate. In this paper, we propose a new learning-based hashing method called ‚Äúfast supervised discrete hashing‚Äù (FSDH) based on ‚Äúsupervised discrete hashing‚Äù (SDH). Regressing the training examples (or hash code) to the corresponding class labels is widely used in ordinary least squares regression. Rather than adopting this method, FSDH uses a very simple yet effective regression of the class labels of training examples to the corresponding hash code to accelerate the algorithm. To the best of our knowledge, this strategy has not previously been used for hashing. Traditional SDH decomposes the optimization into three sub-problems, with the most critical sub-problem - discrete optimization for binary hash codes - solved using iterative discrete cyclic coordinate descent (DCC), which is time-consuming. However, FSDH has a closed-form solution and only requires a single rather than iterative hash code-solving step, which is highly efficient. Furthermore, FSDH is usually faster than SDH for solving the projection matrix for least squares regression, making FSDH generally faster than SDH. For example, our results show that FSDH is about 12-times faster than SDH when the number of hashing bits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than FastHash when the number of hashing bits is 64 on the MNIST data-base. Our experimental results show that FSDH is not only fast, but also outperforms other comparative methods.},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gui, Jie and Liu, Tongliang and Sun, Zhenan and Tao, Dacheng and Tan, Tieniu},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithm design and analysis, Binary codes, Closed-form solutions, Fast supervised discrete hashing, Linear programming, Optimization, Synchronous digital hierarchy, Training, learning-based hashing, least squares regression, supervised discrete hashing},
	pages = {490--496},
}

@article{wang_factor_2010,
	title = {Factor analysis for cross-platform tumor classification based on gene expression profiles},
	volume = {19},
	issn = {0218-1266},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218126610006074},
	doi = {10.1142/S0218126610006074},
	abstract = {Previous studies on tumor classification based on feature extraction from gene expression profiles (GEP) were proven to be effective, but some of such methods lack biomedical meaning to some extent. To deal with this problem, we proposed a novel feature extraction method whose experimental results are of biomedical interpretability and helpful for gaining insight into the structure analysis of gene expression dataset. This method first applied rank sum test to roughly select a set of informative genes and then adopted factor analysis to extract latent factors for tumor classification. Experiments on three pairs of cross-platform tumor datasets indicated that the proposed method can obviously improve the performance of cross-platform classification and only several latent factors, which can represent a large number of informative genes, would obtain very high predictive accuracy on test set. The results also suggested that the classification model trained on one dataset can successfully predict another tumor dataset with the same tumor subtype obtained on different experimental platforms.},
	number = {01},
	urldate = {2021-11-16},
	journal = {Journal of Circuits, Systems and Computers},
	author = {Wang, Shu-Lin and Gui, Jie and Li, Xueling},
	month = feb,
	year = {2010},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Feature extraction, cross-platform analysis, factor analysis, gene expression profiles, tumor classification},
	pages = {243--258},
}

@inproceedings{bonald_minimax_2017,
	title = {A {Minimax} {Optimal} {Algorithm} for {Crowdsourcing}},
	url = {http://arxiv.org/abs/1606.00226},
	abstract = {We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We further prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data sets.},
	urldate = {2021-11-16},
	booktitle = {{arXiv}:1606.00226 [cs, stat]},
	author = {Bonald, Thomas and Combes, Richard},
	month = oct,
	year = {2017},
	note = {arXiv: 1606.00226},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@inproceedings{kleindessner_crowdsourcing_2018,
	title = {Crowdsourcing with {Arbitrary} {Adversaries}},
	url = {https://proceedings.mlr.press/v80/kleindessner18a.html},
	abstract = {Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task. We study a significant extension of this restricted model. We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated. In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude. In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers‚Äô error probabilities.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kleindessner, Matthaeus and Awasthi, Pranjal},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2708--2717},
}

@inproceedings{zhang_label_2017,
	address = {New York, NY, USA},
	series = {{SIGIR} '17},
	title = {Label {Aggregation} for {Crowdsourcing} with {Bi}-{Layer} {Clustering}},
	isbn = {978-1-4503-5022-8},
	url = {https://doi.org/10.1145/3077136.3080679},
	doi = {10.1145/3077136.3080679},
	abstract = {This paper proposes a novel general label aggregation method for both binary and multi-class labeling in crowdsourcing, namely Bi-Layer Clustering (BLC), which clusters two layers of features - the conceptual-level and the physical-level features - to infer true labels of instances. BLC first clusters the instances using the conceptual-level features extracted from their multiple noisy labels and then performs clustering again using the physical-level features. It can facilitate tracking the uncertainty changes of the instances, so that the integrated labels that are likely to be falsely inferred on the conceptual layer can be easily corrected using the estimated labels on the physical layer. Experimental results on two real-world crowdsourcing data sets show that BLC outperforms seven state-of-the-art methods.},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Jing and Sheng, Victor S. and Li, Tao},
	month = aug,
	year = {2017},
	keywords = {clustering, crowdsourcing, inference, label aggregation},
	pages = {921--924},
}

@inproceedings{mallah_plant_2013,
	address = {Innsbruck, Austria},
	title = {Plant {Leaf} {Classification} using {Probabilistic} {Integration} of {Shape}, {Texture} and {Margin} {Features}},
	isbn = {978-0-88986-944-8},
	url = {http://www.actapress.com/PaperInfo.aspx?paperId=455022},
	doi = {10.2316/P.2013.798-098},
	urldate = {2021-11-16},
	booktitle = {Computer {Graphics} and {Imaging} / 798: {Signal} {Processing}, {Pattern} {Recognition} and {Applications}},
	publisher = {ACTAPRESS},
	author = {Mallah, Charles and Cope, James and Orwell, James},
	year = {2013},
}

@article{hunter_mm_2004,
	title = {{MM} algorithms for generalized {Bradley}-{Terry} models},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-1/MM-algorithms-for-generalized-Bradley-Terry-models/10.1214/aos/1079120141.full},
	doi = {10.1214/aos/1079120141},
	abstract = {The Bradley-Terry model for paired comparisons is a simple and much-studied means to describe the probabilities of the possible outcomes when individuals are judged against one another in pairs. Among the many studies of the model in the past 75 years, numerous authors have generalized it in several directions, sometimes providing iterative algorithms for obtaining maximum likelihood estimates for the generalizations. Building on a theory of algorithms known by the initials MM, for minorization-maximization, this paper presents a powerful technique for producing iterative maximum likelihood estimation algorithms or a wide class of generalizations of the Bradley-Terry model. While algorithms for problems of this type have tended to be custom-built in the literature, the techniques in this paper enable their mass production. Simple conditions are stated that guarantee that each algorithm described will produce a sequence that converges to the unique maximum likelihood estimator. Several of the algorithms and convergence results herein are new.},
	number = {1},
	urldate = {2022-02-28},
	journal = {The Annals of Statistics},
	author = {Hunter, David R.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F07, 65D15, Bradley-Terry model, Luce's choice axiom, MM algorithm, Newton-Raphson, Plackett-Luce model, maximum likelihood estimation},
	pages = {384--406},
}

@article{karger_budget-optimal_2014,
	title = {Budget-{Optimal} {Task} {Allocation} for {Reliable} {Crowdsourcing} {Systems}},
	volume = {62},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.2013.1235},
	doi = {10.1287/opre.2013.1235},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Operations Research},
	author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
	month = feb,
	year = {2014},
	pages = {1--24},
}

@article{jiang_class-specific_2019,
	title = {Class-specific attribute weighted naive {Bayes}},
	volume = {88},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318304205},
	doi = {10.1016/j.patcog.2018.11.032},
	abstract = {Due to its easiness to construct and interpret, along with its good performance, naive Bayes (NB) is widely used to address classification problems in real-world applications. In order to alleviate its conditional independence assumption, a mass of attribute weighting approaches have been proposed. However, almost all these approaches assign each attribute a same (global) weight for all classes. In this paper, we call them the general attribute weighting and argue that for NB attribute weighting should be class-specific (class-dependent). Based on this premise, we propose a new paradigm for attribute weighting called the class-specific attribute weighting, which discriminatively assigns each attribute a specific weight for each class. We call the resulting model class-specific attribute weighted naive Bayes (CAWNB). CAWNB selects class-specific attribute weights to maximize the conditional log likelihood (CLL) objective function or minimize the mean squared error (MSE) objective function, and thus two different versions are created, which we denote as CAWNBCLL and CAWNBMSE, respectively. Extensive empirical studies show that CAWNBCLL and CAWNBMSE all obtain more satisfactory experimental results compared with NB and other existing state-of-the-art general attribute weighting approaches. We believe that for NB class-specific attribute weighting could be a more fine-grained attribute weighting approach than general attribute weighting.},
	language = {en},
	urldate = {2021-11-02},
	journal = {Pattern Recognition},
	author = {Jiang, Liangxiao and Zhang, Lungan and Yu, Liangjun and Wang, Dianhong},
	month = apr,
	year = {2019},
	keywords = {Attribute weighting, Naive Bayes, Weight optimization},
	pages = {321--330},
}

@article{li_noise_2019,
	title = {Noise correction to improve data and model quality for crowdsourcing},
	volume = {82},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197619300818},
	doi = {10.1016/j.engappai.2019.04.004},
	abstract = {In supervised learning, obtaining expert labeling of data is expensive and time-consuming in many cases. Crowdsourcing services provide a cheap and efficient way to acquire the labels of data. In crowdsourcing scenario, each instance obtains a multiple noisy label set from multiple different labelers, then ground truth inference algorithms are employed to obtain integrated labels of instances. In spite of the effectiveness of ground truth inference methods, a certain level of class (label) noise still exists in integrated labels, and thus class noise correction methods are dedicated to mitigate the effects of the class noise. However, to our best knowledge, there is little work, up to the present, on exploiting the information in the crowdsourcing scenario. This paper proposes a novel class noise correction method which takes advantage of the information of the multiple noisy label sets. We call our method between-class margin-based noise correction (BMNC). In BMNC, a preliminary filtering is performed before building filters to filter class noise. By the preliminary filtering, some potentially noise instances are first removed. Afterwards, a filter is built to further filter out noise instances. After these two filtering steps, a clean set and a noise set are obtained, and then a classifier is built on the clean set to relabel the instances in noise set. Experimental results on 22 simulated benchmark data sets and eight real-world crowdsourced data sets show that BMNC can significantly reduce the class noise level in integrated labels and thereby enhance the performance of target classifiers.},
	language = {en},
	urldate = {2021-11-16},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Li, Chaoqun and Jiang, Liangxiao and Xu, Wenqiang},
	month = jun,
	year = {2019},
	keywords = {Class noise, Crowdsourcing learning, Integrated labels, Noise correction},
	pages = {184--191},
}

@article{li_noise_2016,
	title = {Noise filtering to improve data and model quality for crowdsourcing},
	volume = {107},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705116301666},
	doi = {10.1016/j.knosys.2016.06.003},
	abstract = {Crowdsourcing services provide an easy means of acquiring labeled training data for supervised learning. However, the labels provided by a single crowd worker are often unreliable. Repeated labeling can be used to solve this problem. After multiple labels have been acquired by repeated labeling for each instance, in general consensus methods are used to obtain the integrated labels of instances. Although consensus methods are effective in practice, it cannot be denied that a level of noise still exists in the set of integrated labels. In this study, an attempt was made to employ noise filters to delete the noise in integrated labels, and consequently, enhance the training data and model quality. In fact, noise handling is a relatively mature field in the machine learning community, and many noise filters for deleting label noise have been presented in the past. However, to the best of our knowledge, in very few studies was noise filtering used to improve crowdsourcing learning. Therefore, in this study we empirically investigated the performance of noise filters in terms of improving crowdsourcing learning. Thus, in this paper some existing noise filters presented in previous papers are reviewed and their experimental application to crowdsourcing learning tasks is described. Experimental results based on 14 benchmark UCI data sets and three real-world data sets show that these noise filters can significantly reduce the noise level in integrated labels and thereby considerably enhance the performance of target classifiers.},
	language = {en},
	urldate = {2021-11-02},
	journal = {Knowledge-Based Systems},
	author = {Li, Chaoqun and Sheng, Victor S. and Jiang, Liangxiao and Li, Hongwei},
	month = sep,
	year = {2016},
	keywords = {Crowdsourcing learning, Integrated labels, Label noise, Noise filtering},
	pages = {96--103},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ‚ÄúImageNet‚Äù, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@article{wu_multi-label_2020,
	title = {Multi-{Label} {Active} {Learning} {Algorithms} for {Image} {Classification}: {Overview} and {Future} {Promise}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Multi-{Label} {Active} {Learning} {Algorithms} for {Image} {Classification}},
	url = {https://dl.acm.org/doi/10.1145/3379504},
	doi = {10.1145/3379504},
	abstract = {Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.},
	language = {en},
	number = {2},
	urldate = {2021-11-16},
	journal = {ACM Computing Surveys},
	author = {Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng},
	month = jul,
	year = {2020},
	pages = {1--35},
}

@inproceedings{donmez_efficiently_2009,
	address = {Paris, France},
	title = {Efficiently learning the accuracy of labeling sources for selective sampling},
	isbn = {978-1-60558-495-9},
	url = {http://portal.acm.org/citation.cfm?doid=1557019.1557053},
	doi = {10.1145/1557019.1557053},
	abstract = {Many scalable data mining tasks rely on active learning to provide the most useful accurately labeled instances. However, what if there are multiple labeling sources (‚Äòoracles‚Äô or ‚Äòexperts‚Äô) with diÔ¨Äerent but unknown reliabilities? With the recent advent of inexpensive and scalable online annotation tools, such as Amazon‚Äôs Mechanical Turk, the labeling process has become more vulnerable to noise - and without prior knowledge of the accuracy of each individual labeler. This paper addresses exactly such a challenge: how to jointly learn the accuracy of labeling sources and obtain the most informative labels for the active learning task at hand minimizing total labeling eÔ¨Äort. More speciÔ¨Åcally, we present IEThresh (Interval Estimate Threshold) as a strategy to intelligently select the expert(s) with the highest estimated labeling accuracy. IEThresh estimates a conÔ¨Ådence interval for the reliability of each expert and Ô¨Ålters out the one(s) whose estimated upper-bound conÔ¨Ådence interval is below a threshold - which jointly optimizes expected accuracy (mean) and need to better estimate the expert‚Äôs accuracy (variance). Our framework is Ô¨Çexible enough to work with a wide range of diÔ¨Äerent noise levels and outperforms baselines such as asking all available experts and random expert selection. In particular, IEThresh achieves a given level of accuracy with less than half the queries issued by allexperts labeling and less than a third the queries required by random expert selection on datasets such as the UCI mushroom one. The results show that our method naturally balances exploration and exploitation as it gains knowledge of which experts to rely upon, and selects them with increasing frequency.},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '09},
	publisher = {ACM Press},
	author = {Donmez, Pinar and Carbonell, Jaime G. and Schneider, Jeff},
	year = {2009},
	keywords = {active learning, estimation, labeler selection, noisy labelers},
	pages = {259},
}

@article{tian_max-margin_2019,
	title = {Max-{Margin} {Majority} {Voting} for {Learning} from {Crowds}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2860987},
	abstract = {Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting (M3V) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices for different application settings. We first introduce the crowdsourcing margin of majority voting, then we formulate the joint learning as a regularized Bayesian inference (RegBayes) problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and M3V as its two special cases. Due to the flexibility of our model, we extend it to handle crowdsourced labels with an ordinal structure with the main ideas about the crowdsourcing margin unchanged. Moreover, we consider an online learning-from-crowds setting where labels coming in a stream. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tian, Tian and Zhu, Jun and Qiaoben, You},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Bayes methods, Crowdsourcing, Labeling, Max-margin learning, Maximum likelihood estimation, Noise measurement, Task analysis, crowdsourcing, online learning, regularized Bayesian inference},
	pages = {2480--2494},
}

@inproceedings{tu_multi-label_2018,
	title = {Multi-label {Answer} {Aggregation} {Based} on {Joint} {Matrix} {Factorization}},
	doi = {10.1109/ICDM.2018.00067},
	abstract = {Crowdsourcing is a useful and economic approach to data annotation. To obtain annotation of high quality, various aggregation approaches have been developed, which take into account different factors that impact the quality of aggregated answers. However, existing methods generally focus on single-label (multi-class and binary) tasks, and they ignore the inter-correlation between labels, and thus may have compromised quality. In this paper, we introduce a Multi-Label answer aggregation approach based on Joint Matrix Factorization (ML-JMF). ML-JMF selectively and jointly factorizes the sample-label association matrices collected from different annotators into products of individual and shared low-rank matrices. As such, it takes advantage of the robustness of low-rank matrix approximation to noise, and reduces the impact of unreliable annotators by assigning small (zero) weights to their annotation matrices. In addition, it takes advantage of the correlation among labels by leveraging the shared low-rank matrix, and of the similarity between annotators using the individual low-rank matrices to guide the factorization. ML-JMF pursues the low-rank matrices via a unified objective function, and introduces an iterative technique to optimize it. ML-JMF finally uses the optimized low-rank matrices and weights to infer the ground-truth labels. Our experimental results on multi-label datasets show that ML-JMF outperforms competitive methods in inferring ground truth labels. Our approach can identify unreliable annotators, and is robust against their misleading answers through the assignment of small (zero) weights to their annotation.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Tu, Jinzheng and Yu, Guoxian and Domeniconi, Carlotta and Wang, Jun and Xiao, Guoqiang and Guo, Maozu},
	month = nov,
	year = {2018},
	note = {ISSN: 2374-8486},
	keywords = {Correlation, Crowdsourcing, Crowdsourcing, Multi-Label Learning, Joint Matrix Factorization, Spammers, Image annotation, Labeling, Noise measurement, Reliability, Task analysis},
	pages = {517--526},
}

@inproceedings{ma_faitcrowd_2015,
	address = {New York, NY, USA},
	series = {{KDD} '15},
	title = {{FaitCrowd}: {Fine} {Grained} {Truth} {Discovery} for {Crowdsourced} {Data} {Aggregation}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {{FaitCrowd}},
	url = {https://doi.org/10.1145/2783258.2783314},
	doi = {10.1145/2783258.2783314},
	abstract = {In crowdsourced data aggregation task, there exist conflicts in the answers provided by large numbers of sources on the same set of questions. The most important challenge for this task is to estimate source reliability and select answers that are provided by high-quality sources. Existing work solves this problem by simultaneously estimating sources' reliability and inferring questions' true answers (i.e., the truths). However, these methods assume that a source has the same reliability degree on all the questions, but ignore the fact that sources' reliability may vary significantly among different topics. To capture various expertise levels on different topics, we propose FaitCrowd, a fine grained truth discovery model for the task of aggregating conflicting data collected from multiple users/sources. FaitCrowd jointly models the process of generating question content and sources' provided answers in a probabilistic model to estimate both topical expertise and true answers simultaneously. This leads to a more precise estimation of source reliability. Therefore, FaitCrowd demonstrates better ability to obtain true answers for the questions compared with existing approaches. Experimental results on two real-world datasets show that FaitCrowd can significantly reduce the error rate of aggregation compared with the state-of-the-art multi-source aggregation approaches due to its ability of learning topical expertise from question content and collected answers.},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Fenglong and Li, Yaliang and Li, Qi and Qiu, Minghui and Gao, Jing and Zhi, Shi and Su, Lu and Zhao, Bo and Ji, Heng and Han, Jiawei},
	month = aug,
	year = {2015},
	keywords = {Crowdsourcing, source reliability, truth discovery},
	pages = {745--754},
}

@article{yin_truth_2020,
	title = {Truth {Inference} {With} a {Deep} {Clustering}-{Based} {Aggregation} {Model}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8950460/},
	doi = {10.1109/ACCESS.2020.2964484},
	urldate = {2021-11-16},
	journal = {IEEE Access},
	author = {Yin, Li'ang and Liu, Yunfei and Zhang, Weinan and Yu, Yong},
	year = {2020},
	pages = {16662--16675},
}

@article{zhong_quality-sensitive_2017,
	title = {A {Quality}-{Sensitive} {Method} for {Learning} from {Crowds}},
	volume = {29},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2017.2738643},
	abstract = {In real-world applications, the oracle who can label all instances correctly may not exist or may be too expensive to acquire. Alternatively, crowdsourcing provides an easy way to get labels at a low cost from multiple non-expert annotators. During the past few years, much attention has been paid to learning from such crowdsourcing data, namely Learning from Crowds (LFC). Despite their proper statistical foundations, the existing methods for LFC still suffer from several disadvantages, such as needing prior knowledge to select the expertise model to represent the behavior of annotators, involving non-convex optimization problems, or restricting the classifier type being used. This paper addresses LFC from a quality-sensitive perspective and presents a novel framework named QS-LFC. Through reformulating the original LFC problem as a quality-sensitive learning problem, the above-mentioned disadvantages of existing methods can be avoided. Further, a support vector machine (SVM) implementation of QS-LFC is proposed. Experimental results on both synthetic and real-world data sets demonstrate that QS-LFC can achieve better generalization performance and is more robust to the noisy labels, than the existing methods.},
	number = {12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhong, Jinhong and Yang, Peng and Tang, Ke},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Crowdsourcing, Learning from crowds, Learning systems, Noise measurement, Optimization, Reliability, Supervised learning, Support vector machines, crowdsourcing, learning from multiple annotators, quality-sensitive learning},
	pages = {2643--2654},
}

@article{alqershi_robust_2020,
	title = {A {Robust} {Consistency} {Model} of {Crowd} {Workers} in {Text} {Labeling} {Tasks}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9187781/},
	doi = {10.1109/ACCESS.2020.3022773},
	urldate = {2021-11-16},
	journal = {IEEE Access},
	author = {Alqershi, Fattoh and Al-Qurishi, Muhammad and Aksoy, Mehmet Sabih and Alrubaian, Majed and Imran, Muhammad},
	year = {2020},
	pages = {168381--168393},
}

@inproceedings{rizos_average_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Average {Jane}, {Where} {Art} {Thou}? ‚Äì {Recent} {Avenues} in {Efficient} {Machine} {Learning} {Under} {Subjectivity} {Uncertainty}},
	isbn = {978-3-030-50146-4},
	shorttitle = {Average {Jane}, {Where} {Art} {Thou}?},
	doi = {10.1007/978-3-030-50146-4_4},
	abstract = {In machine learning tasks an actual ‚Äòground truth‚Äô may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the learning task is, as human agreement can be low. To cope with the resulting high uncertainty, one could train individual models reflecting a single human‚Äôs opinion. However, this is not viable, if one aims at mirroring the general opinion of a hypothetical ‚Äòcompletely average person‚Äô ‚Äì the ‚Äòaverage Jane‚Äô. Here, I summarise approaches to optimally learn efficiently in such a case. First, different strategies of reaching a single learning target from several labellers will be discussed. This includes varying labeller trustability and the case of time-continuous labels with potential dynamics. As human labelling is a labour-intensive endeavour, active and cooperative learning strategies can help reduce the number of labels needed. Next, sample informativeness can be exploited in teacher-based algorithms to additionally weigh data by certainty. In addition, multi-target learning of different labeller tracks in parallel and/or of the uncertainty can help improve the model robustness and provide an additional uncertainty measure. Cross-modal strategies to reduce uncertainty offer another view. From these and further recent strategies, I distil a number of future avenues to handle subjective uncertainty in machine learning. These comprise bigger, yet weakly labelled data processing basing amongst other on reinforcement learning, lifelong learning, and self-learning. Illustrative examples stem from the fields of Affective Computing and Digital Health ‚Äì both notoriously marked by subjectivity uncertainty.},
	language = {en},
	booktitle = {Information {Processing} and {Management} of {Uncertainty} in {Knowledge}-{Based} {Systems}},
	publisher = {Springer International Publishing},
	author = {Rizos, Georgios and Schuller, Bj√∂rn W.},
	editor = {Lesot, Marie-Jeanne and Vieira, Susana and Reformat, Marek Z. and Carvalho, Jo√£o Paulo and Wilbik, Anna and Bouchon-Meunier, Bernadette and Yager, Ronald R.},
	year = {2020},
	keywords = {Active learning, Cooperative learning, Machine learning, Subjectivity, Uncertainty},
	pages = {42--55},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {1566-2535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
	doi = {10.1016/j.inffus.2021.05.008},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
	language = {en},
	urldate = {2022-06-14},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	keywords = {Artificial intelligence, Bayesian statistics, Deep learning, Ensemble learning, Machine learning, Uncertainty quantification},
	pages = {243--297},
}

@inproceedings{zhang_multi-label_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Multi-{Label} {Inference} for {Crowdsourcing}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3219958},
	doi = {10.1145/3219819.3219958},
	abstract = {When acquiring labels from crowdsourcing platforms, a task may be designed to include multiple labels and the values of each label may belong to a set of various distinct options, which is the so-called multi-class multi-label annotation. To improve the quality of labels, one task is independently completed by a group of heterogeneous crowdsourced workers. Then, the true values of the multiple labels of each task are inferred from these repeated noisy labels. In this paper, we propose a novel probabilistic method, which includes a multi-class multi-label dependency (MCMLD) model, to address this problem. The proposed method assumes that the label-correlation exists in both unknown true labels and noisy crowdsourced labels. Thus, it introduces a mixture of multiple independently multinoulli distributions to capture the correlation among the labels. Finally, the unknown true values of the multiple labels of each task, together with a set of confusion matrices modeling the reliability of the workers, can be jointly inferred through an EM algorithm. Experiments with three simulated typical crowdsourcing scenarios and a real-world dataset consistently show that our proposed MCMLD method significantly outperforms several competitive alternatives. Furthermore, if the labels are strongly correlated, the advantage of MCMLD will be more remarkable.},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Jing and Wu, Xindong},
	month = jul,
	year = {2018},
	keywords = {Crowdsourcing, Maximum likelihood estimation, label aggregation, mixture models, probabilistic graphical models},
	pages = {2738--2747},
}

@inproceedings{bi_learning_2014,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'14},
	title = {Learning to predict from crowdsourced data},
	isbn = {978-0-9749039-1-0},
	abstract = {Crowdsourcing services like Amazon's Mechanical Turk have facilitated and greatly expedited the manual labeling process from a large number of human workers. However, spammers are often unavoidable and the crowdsourced labels can be very noisy. In this paper, we explicitly account for four sources for a noisy crowdsourced label: worker's dedication to the task, his/her expertise, his/her default labeling judgement, and sample difficulty. A novel mixture model is employed for worker annotations, which learns a prediction model directly from samples to labels for efficient out-of-sample testing. Experiments on both simulated and real-world crowdsourced data sets show that the proposed method achieves significant improvements over the state-of-the-art.},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the {Thirtieth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Bi, Wei and Wang, Liwei and Kwok, James T. and Tu, Zhuowen},
	month = jul,
	year = {2014},
	pages = {82--91},
}

@article{dawid_maximum_1979,
	title = {Maximum {Likelihood} {Estimation} of {Observer} {Error}-{Rates} {Using} the {EM} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	url = {https://www.jstor.org/stable/2346806},
	doi = {10.2307/2346806},
	abstract = {In compiling a patient record many facets are subject to errors of measurement. A model is presented which allows individual error-rates to be estimated for polytomous facets even when the patient's "true" response is not available. The EM algorithm is shown to provide a slow but sure way of obtaining maximum likelihood estimates of the parameters of interest. Some preliminary experience is reported and the limitations of the method are described.},
	number = {1},
	urldate = {2021-11-16},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Dawid, A. P. and Skene, A. M.},
	year = {1979},
	note = {Publisher: [Wiley, Royal Statistical Society]},
	pages = {20--28},
}

@article{tao_label_2020,
	title = {Label similarity-based weighted soft majority voting and pairing for crowdsourcing},
	volume = {62},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-020-01475-y},
	doi = {10.1007/s10115-020-01475-y},
	abstract = {Crowdsourcing services provide an efficient and relatively inexpensive approach to obtain substantial amounts of labeled data by employing crowd workers. It is obvious that the labeling qualities of crowd workers directly affect the quality of the labeled data. However, existing label aggregation strategies seldom consider the differences in the quality of workers labeling different instances. In this paper, we argue that a single worker may even have different labeling qualities on different instances. Based on this premise, we propose four new strategies by assigning different weights to workers when labeling different instances. In our proposed strategies, we first use the similarity among worker labels to estimate the specific quality of the worker on different instances, and then we build a classifier to estimate the overall quality of the worker across all instances. Finally, we combine these two qualities to define the weight of the worker labeling a particular instance. Extensive experimental results show that our proposed strategies significantly outperform other existing state-of-the-art label aggregation strategies.},
	language = {en},
	number = {7},
	urldate = {2021-10-18},
	journal = {Knowledge and Information Systems},
	author = {Tao, Fangna and Jiang, Liangxiao and Li, Chaoqun},
	month = jul,
	year = {2020},
	keywords = {Crowdsourcing, Label aggregation, Label similarity, Overall quality, Specific quality},
	pages = {2521--2538},
}

@misc{lichman_uci_2017,
	title = {{UCI} {Machine} {Learning} {Repository}. {University} of {California}, {School} of {Information} and {Computer} {Science}, {Irvine}, {CA} (2013)},
	author = {Lichman, M.},
	year = {2017},
}

@inproceedings{li_neural_2020,
	address = {Barcelona, Spain (Online)},
	title = {A {Neural} {Model} for {Aggregating} {Coreference} {Annotation} in {Crowdsourcing}},
	url = {https://aclanthology.org/2020.coling-main.507},
	doi = {10.18653/v1/2020.coling-main.507},
	abstract = {Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a high-performance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention's annotation complexity and annotators' reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model's interpretability through a comprehensive analysis of experimental results.},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Li, Maolin and Takamura, Hiroya and Ananiadou, Sophia},
	month = dec,
	year = {2020},
	pages = {5760--5773},
}

@article{li_confidence-aware_2014,
	title = {A confidence-aware approach for truth discovery on long-tail data},
	volume = {8},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2735496.2735505},
	doi = {10.14778/2735496.2735505},
	abstract = {In many real world applications, the same item may be described by multiple sources. As a consequence, conÔ¨Çicts among these sources are inevitable, which leads to an important task: how to identify which piece of information is trustworthy, i.e., the truth discovery task. Intuitively, if the piece of information is from a reliable source, then it is more trustworthy, and the source that provides trustworthy information is more reliable. Based on this principle, truth discovery approaches have been proposed to infer source reliability degrees and the most trustworthy information (i.e., the truth) simultaneously. However, existing approaches overlook the ubiquitous long-tail phenomenon in the tasks, i.e., most sources only provide a few claims and only a few sources make plenty of claims, which causes the source reliability estimation for small sources to be unreasonable. To tackle this challenge, we propose a conÔ¨Ådence-aware truth discovery (CATD) method to automatically detect truths from conÔ¨Çicting data with long-tail phenomenon. The proposed method not only estimates source reliability, but also considers the conÔ¨Ådence interval of the estimation, so that it can effectively reÔ¨Çect real source reliability for sources with various levels of participation. Experiments on four real world tasks as well as simulated multi-source long-tail datasets demonstrate that the proposed method outperforms existing state-of-the-art truth discovery approaches by successful discounting the effect of small sources.},
	language = {en},
	number = {4},
	urldate = {2021-11-16},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Qi and Li, Yaliang and Gao, Jing and Su, Lu and Zhao, Bo and Demirbas, Murat and Fan, Wei and Han, Jiawei},
	month = dec,
	year = {2014},
	pages = {425--436},
}

@article{li_unified_2021,
	title = {A unified task recommendation strategy for realistic mobile crowdsourcing system},
	volume = {857},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S030439752030757X},
	doi = {10.1016/j.tcs.2020.12.034},
	abstract = {A well-designed task recommendation framework aims to protect the data quality as well as increase the task execution results. However, current crowdsourcing systems ignore the fact that there are few duplicate task expectations because of the budget limitation in realistic conditions. Besides, a practical crowdsourcing system needs to recommend new tasks without previous knowledge about the concrete task content due to short task lifespan. Thus, most of the existing studies are not applicable due to the idealized assumptions. In this paper, we formally define the problem and prove it is NP-Hard. For the problem, we design a unified task recommendation system for realistic conditions to address the mentioned problems, Pioneer-Assisted Task RecommendatiON (PATRON) framework. The framework first selects a set of pioneer workers to collect initial knowledge of the new tasks. Then it adopts the k-medoids clustering algorithm to split the workers into subsets based on the worker similarity. Cluster selection and worker pruning provides accurate and efficient recommendations that satisfy the valid recommendation requirements from requesters. Finally, we conducted our experiments based on real datasets from a famous Chinese crowdsourcing platform, Tencent SOHO. The experimental results show the efficiency and accuracy of PATRON compared with three baseline methods from several perspectives, such as recommendation success rate and recommended worker quality.},
	language = {en},
	urldate = {2021-11-16},
	journal = {Theoretical Computer Science},
	author = {Li, Zhiyao and Cheng, Bosen and Gao, Xiaofeng and Chen, Huai and Chen, Guihai},
	month = feb,
	year = {2021},
	keywords = {Crowdsourcing, K-medoids, Recommendation system},
	pages = {43--58},
}

@article{li_double_2019,
	title = {Double weighted {K}-nearest voting for label aggregation in crowdsourcing learning},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-019-08054-6},
	doi = {10.1007/s11042-019-08054-6},
	abstract = {In this article, we propose a new voting strategy in crowdsourcing learning by avoiding the effects of missing labels and poor workers. To achieve this, we apply K-nearest neighbor idea and fitting learning to consider the importance of neighbor sample labels and the importance of workers, respectively. Specifically, we apply different weights to different neighbors based on the distance of the sample neighbors, which is important for the neighbor sample labels. At the same time, we propose an effective worker model to consider the importance of workers by removing redundant workers. In addition, we use an alternate iterative optimization algorithm to solve our proposed model. The experimental results show that the proposed algorithm achieves better performance in label aggregation accuracy than the comparison algorithm.},
	language = {en},
	number = {23},
	urldate = {2021-11-16},
	journal = {Multimedia Tools and Applications},
	author = {Li, Jiaye and Yu, Hao and Zhang, Leyuan and Wen, Guoqiu},
	month = dec,
	year = {2019},
	pages = {33357--33374},
}

@inproceedings{romero_open_2015,
	address = {Cartagena de Indias, Colombia},
	title = {An open access thyroid ultrasound image database},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2073532},
	doi = {10.1117/12.2073532},
	urldate = {2022-05-16},
	author = {Pedraza, Lina and Vargas, Carlos and Narv√°ez, Fabi√°n and Dur√°n, Oscar and Mu√±oz, Emma and Romero, Eduardo},
	editor = {Romero, Eduardo and Lepore, Natasha},
	month = jan,
	year = {2015},
	pages = {92870W},
}

@article{zhao_automatic_2022,
	title = {Automatic {Thyroid} {Ultrasound} {Image} {Classification} {Using} {Feature} {Fusion} {Network}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9725813/},
	doi = {10.1109/ACCESS.2022.3156096},
	urldate = {2022-05-16},
	journal = {IEEE Access},
	author = {Zhao, Xiaohui and Shen, Xueqin and Wan, Wenbo and Lu, Yuanyuan and Hu, Shidong and Xiao, Ruoxiu and Du, Xiaohui and Li, Junlai},
	year = {2022},
	pages = {27917--27924},
}

@article{wang_deep_2021,
	title = {Deep {High}-{Resolution} {Representation} {Learning} for {Visual} {Recognition}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9052469/},
	doi = {10.1109/TPAMI.2020.2983686},
	number = {10},
	urldate = {2022-05-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
	month = oct,
	year = {2021},
	pages = {3349--3364},
}

@incollection{ferrari_encoder-decoder_2018,
	address = {Cham},
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	url = {http://link.springer.com/10.1007/978-3-030-01234-2_49},
	urldate = {2022-05-16},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_49},
	pages = {833--851},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2022-05-16},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {Number: arXiv:1409.1556
arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_recent_2020,
	title = {Recent advances in deep learning for object detection},
	volume = {396},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220301430},
	doi = {10.1016/j.neucom.2020.01.085},
	abstract = {Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications \& benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.},
	language = {en},
	urldate = {2022-05-16},
	journal = {Neurocomputing},
	author = {Wu, Xiongwei and Sahoo, Doyen and Hoi, Steven C. H.},
	month = jul,
	year = {2020},
	keywords = {Deep convolutional neural networks, Deep learning, Object detection},
	pages = {39--64},
}

@article{liu_deep_2019,
	title = {Deep {Learning} in {Medical} {Ultrasound} {Analysis}: {A} {Review}},
	volume = {5},
	issn = {2095-8099},
	shorttitle = {Deep {Learning} in {Medical} {Ultrasound} {Analysis}},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809918301887},
	doi = {10.1016/j.eng.2018.11.020},
	abstract = {Ultrasound (US) has become one of the most commonly performed imaging modalities in clinical practice. It is a rapidly evolving technology with certain advantages and with unique challenges that include low imaging quality and high variability. From the perspective of image analysis, it is essential to develop advanced automatic US image analysis methods to assist in US diagnosis and/or to make such assessment more objective and accurate. Deep learning has recently emerged as the leading machine learning tool in various research fields, and especially in general imaging analysis and computer vision. Deep learning also shows huge potential for various automatic US image analysis tasks. This review first briefly introduces several popular deep learning architectures, and then summarizes and thoroughly discusses their applications in various specific tasks in US image analysis, such as classification, detection, and segmentation. Finally, the open challenges and potential trends of the future application of deep learning in medical US image analysis are discussed.},
	language = {en},
	number = {2},
	urldate = {2022-05-16},
	journal = {Engineering},
	author = {Liu, Shengfeng and Wang, Yi and Yang, Xin and Lei, Baiying and Liu, Li and Li, Shawn Xiang and Ni, Dong and Wang, Tianfu},
	month = apr,
	year = {2019},
	keywords = {Classification, Deep learning, Detection, Medical ultrasound analysis, Segmentation},
	pages = {261--275},
}

@article{ren_faster_2015,
	title = {Faster r-cnn: {Towards} real-time object detection with region proposal networks},
	volume = {28},
	shorttitle = {Faster r-cnn},
	journal = {Advances in neural information processing systems},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
}

@misc{bahrampour_comparative_2016,
	title = {Comparative {Study} of {Deep} {Learning} {Software} {Frameworks}},
	url = {http://arxiv.org/abs/1511.06435},
	abstract = {Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of five deep learning frameworks, namely Caffe, Neon, TensorFlow, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Caffe is the easiest for evaluating the performance of standard deep architectures. Finally, TensorFlow is a very flexible framework, similar to Theano, but its performance is currently not competitive compared to the other studied frameworks.},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
	month = mar,
	year = {2016},
	note = {Number: arXiv:1511.06435
arXiv:1511.06435 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {Number: arXiv:1207.0580
arXiv:1207.0580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{abdolali_automated_2020,
	title = {Automated thyroid nodule detection from ultrasound imaging using deep convolutional neural networks},
	volume = {122},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482520302262},
	doi = {10.1016/j.compbiomed.2020.103871},
	abstract = {Thyroid cancer is the most common endocrine cancer and its incidence has continuously increased worldwide. In this paper, we focus on the challenging problem of nodule detection from ultrasound scans. In current clinical practice, this task is performed manually, which is tedious, subjective and highly depends on the clinical experience of radiologists. We propose a novel deep neural network architecture with carefully designed loss function regularization, and network hyperparameters to perform nodule detection without complex post-processing refinement steps. The local training and validation datasets consist of 2461 and 820 ultrasound frames acquired from 60 and 20 patients with a high degree of variability, respectively. The core of the proposed method is a deep learning framework based on multi-task model Mask R-CNN. We have developed a loss function with regularization that prioritizes detection over segmentation. Validation was conducted for 821 ultrasound frames from 20 patients. The proposed model can detect various types of thyroid nodules. The experimental results indicate that our proposed method is effective in thyroid nodule detection. Comparisons with the results by Faster R-CNN and conventional Mask R-CNN demonstrate that the proposed model outperforms the prior state-of-the-art detection methods.},
	language = {en},
	urldate = {2022-05-16},
	journal = {Computers in Biology and Medicine},
	author = {Abdolali, Fatemeh and Kapur, Jeevesh and Jaremko, Jacob L. and Noga, Michelle and Hareendranathan, Abhilash R. and Punithakumar, Kumaradevan},
	month = jul,
	year = {2020},
	keywords = {Computer aided diagnosis, Convolutional neural network, Deep learning, Mask R-CNN, Thyroid nodules, Ultrasound},
	pages = {103871},
}

@article{hoang_interobserver_2018,
	title = {Interobserver {Variability} of {Sonographic} {Features} {Used} in the {American} {College} of {Radiology} {Thyroid} {Imaging} {Reporting} and {Data} {System}},
	volume = {211},
	issn = {1546-3141},
	doi = {10.2214/AJR.17.19192},
	abstract = {OBJECTIVE: The purpose of this study was to assess interobserver variability in assigning features in the American College of Radiology Thyroid Imaging Reporting and Data System (ACR TI-RADS) lexicon and in making recommendations for thyroid nodule biopsy.
MATERIALS AND METHODS: The study cohort comprised 100 nodules in 92 patients who underwent fine-needle aspiration with definitive cytologic results (Bethesda category II or VI) or diagnostic lobectomy between April 2009 and May 2010. Eight board-certified radiologists evaluated the nodules according to the five feature categories that constitute ACR TI-RADS and gave a biopsy recommendation based on their own practice. Variability in feature assignment and biopsy recommendation was assessed with the Fleiss kappa statistic.
RESULTS: Agreement in interpretation was fair to moderate for all features except shape (Œ∫ = 0.61) and macrocalcifications (Œ∫ = 0.73), which had substantial agreement. The features with the poorest agreement were margin and other types of echogenic foci, which had kappa values ranging from 0.25 to 0.39, indicating fair agreement. Interobserver agreement regarding biopsy recommendation was fair (Œ∫ = 0.22) based on radiologists' current practice. Applying ACR TI-RADS resulted in moderate agreement (Œ∫ = 0.51).
CONCLUSION: Variability in interpreting thyroid nodule sonographic features was highest for margin and all types of echogenic foci, except for macrocalcifications. Because radiologists' interpretations of these features change the level of suspicion of thyroid malignancy, the results of this study suggest a need for further education. Despite the variability in assigning features, adoption of ACR TI-RADS improves agreement for recommending biopsy.},
	language = {eng},
	number = {1},
	journal = {AJR. American journal of roentgenology},
	author = {Hoang, Jenny K. and Middleton, William D. and Farjat, Alfredo E. and Teefey, Sharlene A. and Abinanti, Nicole and Boschini, Fernando J. and Bronner, Abraham J. and Dahiya, Nirvikar and Hertzberg, Barbara S. and Newman, Justin R. and Scanga, Daniel and Vogler, Robert C. and Tessler, Franklin N.},
	month = jul,
	year = {2018},
	pmid = {29702015},
	keywords = {Adult, Aged, Aged, 80 and over, Biopsy, Fine-Needle, Female, Humans, Male, Middle Aged, Observer Variation, Societies, Medical, TI-RADS, Thyroid Neoplasms, Thyroid Nodule, Ultrasonography, United States, interobserver agreement, interobserver variability, thyroid malignancy, thyroid ultrasound},
	pages = {162--167},
}

@article{zheng_automated_2022,
	title = {Automated detection and recognition of thyroid nodules in ultrasound images using {Improve} {Cascade} {Mask} {R}-{CNN}},
	volume = {81},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-10939-4},
	doi = {10.1007/s11042-021-10939-4},
	abstract = {Accurate diagnosis of thyroid nodules using ultrasonography heavily relies on the superb skills and rich experience of senior radiologists, considering the low contrast, high noise of the ultrasound image, and the diverse appearance of the nodules. Computer-aided diagnosis systems could diagnose thyroid nodules based on ultrasound characteristics to assist radiologists. However, the existing learning-based approaches for detecting and recognizing thyroid nodules have the problems of inaccurate localization and low recognition accuracy. In this study, we propose an Improved Cascade Mask R-CNN for effectively detecting and recognizing thyroid nodules. Firstly, a more effective detector is designed to better classify the ROIs and better correct the bounding boxes. Secondly, a more effective balanced L1 loss function is used to increase the gradient of the easy sample and solve the problem of imbalance between hard samples and easy samples¬†during training. Finally, a more¬†effective soft non-maximum suppression (Soft-NMS) method is used to set an attenuation function for adjacent bounding boxes, which solves the problem of possible missing detection in non-maximum suppression (NMS). The improved model is trained and verified by using real 1408 images collected from the known hospital. Under the localization accuracy of the IoU threshold of 0.5, the mAP reaches 87.1\%, and the recognition accuracy reaches 98.67\%. The experiment results show that the improved model is¬†effective and highly valuable to help the doctors for the recognition of benign and malignant thyroid nodules.},
	language = {en},
	number = {10},
	urldate = {2022-05-16},
	journal = {Multimedia Tools and Applications},
	author = {Zheng, Yinghao and Qin, Lina and Qiu, Taorong and Zhou, Aiyun and Xu, Pan and Xue, Zhixin},
	month = apr,
	year = {2022},
	keywords = {Automatic detection, Improve Cascade Mask R-CNN, Recognition, Thyroid nodule, Ultrasound image},
	pages = {13253--13273},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Deep learning, Feature extraction, Neural networks, Object detection, Task analysis, Training, neural network, object detection},
	pages = {3212--3232},
}

@article{chen_review_2020,
	title = {A review of thyroid gland segmentation and thyroid nodule segmentation methods for medical ultrasound images},
	volume = {185},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260719308454},
	doi = {10.1016/j.cmpb.2020.105329},
	abstract = {Background and objective Thyroid image segmentation is an indispensable part in computer-aided diagnosis systems and medical image diagnoses of thyroid diseases. There have been dozens of studies on thyroid gland segmentation and thyroid nodule segmentation in ultrasound images. The aim of this work is to categorize and review the thyroid gland segmentation and thyroid nodule segmentation methods in medical ultrasound. Methods This work proposes a categorization approach of thyroid gland segmentation and thyroid nodule segmentation methods according to the theoretical bases of segmentation methods. The segmentation methods are categorized into four groups, including contour and shape based methods, region based methods, machine and deep learning methods and hybrid methods. The representative articles are reviewed with detailed descriptions of methods and analyses of correlations between methods. The evaluation metrics for the reviewed segmentation methods are named uniformly in this work. The segmentation performance results using the uniformly named evaluation metrics are compared. Results After careful investigation, 28 representative papers are selected for comprehensive analyses and comparisons in this review. The dominant thyroid gland segmentation methods are machine and deep learning methods. The training of massive data makes these models have better segmentation performance and robustness. But deep learning models usually require plenty of marked training data and long training time. For thyroid nodule segmentation, the most common methods are contour and shape based methods, which have good segmentation performance. However, most of them are tested on small datasets. Conclusions Based on the comprehensive consideration of application scenario, image features, method practicability and segmentation performance, the appropriate segmentation method for specific situation can be selected. Furthermore, several limitations of current thyroid ultrasound image segmentation methods are presented, which may be overcome in future studies, such as the segmentation of pathological or abnormal thyroid glands, identification of the specific nodular diseases, and the standard thyroid ultrasound image datasets.},
	language = {en},
	urldate = {2022-05-16},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Chen, Junying and You, Haijun and Li, Kai},
	month = mar,
	year = {2020},
	keywords = {Gland segmentation method, Nodule segmentation method, Segmentation performance analysis, Thyroid ultrasound image},
	pages = {105329},
}

@article{song_dual-branch_2022,
	title = {Dual-branch network via pseudo-label training for thyroid nodule detection in ultrasound image},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-021-02967-2},
	doi = {10.1007/s10489-021-02967-2},
	abstract = {Automated nodule detection in the ultrasound image is essential for computer-aided thyroid tumor diagnosis. However, in the ultrasound image, the solid nodule has imaging characteristics similar to other tissues, making it challenging to detect such nodules. Therefore, we proposed a feature-enhanced dual-branch network (FDnet) to complete the nodule detection task by adding a semantic segmentation branch and a feature enhancement mechanism into the detection network. This design improves the target area‚Äôs proposal score and suppresses the interference of similar tissues, which can reduce the false-positive rate of the proposed bounding box and finally obtain more reliable detection results. Additionally, to solve the lack of fine-grained mask information for semantic segmentation branch training in the actual scenario, we also proposed an iterative training strategy that combines the ground-truth boundary box with the branch results to generate a pseudo-label mask. Finally, we carried out various comparative experiments to verify the feasibility of the proposed network and training strategy. A series of experiments showed that FDnet could achieve competitive detection performance (mAP: 61.8/92.5/65.9), which metrics are better than the state-of-the-art detection methods. Besides, the performance using the pseudo-label mask training is also close to using the ground-truth mask in the public dataset, and the inference speed per image is also comparable to that of other networks both in the two datasets. This result shows that our method can improve the efficiency of thyroid nodule detection without fine-grained annotation, and the output result of the trained semantic segmentation branch can guide the further segmentation of nodule edge, which has practical clinical significance. We will release the source code and the public dataset at https://github.com/songruoning/Thyroid\_Solid\_Nodule.},
	language = {en},
	urldate = {2022-05-16},
	journal = {Applied Intelligence},
	author = {Song, Ruoning and Zhu, Chuang and Zhang, Long and Zhang, Tong and Luo, Yihao and Liu, Jun and Yang, Jie},
	month = jan,
	year = {2022},
	keywords = {Dual-branch network, Multi-task joint training, Pseudo-label mask, Thyroid nodule detection, Ultrasound image},
}

@article{buda_management_2019,
	title = {Management of thyroid nodules seen on {US} images: deep learning may match performance of radiologists},
	volume = {292},
	shorttitle = {Management of thyroid nodules seen on {US} images},
	number = {3},
	journal = {Radiology},
	author = {Buda, Mateusz and Wildman-Tobriner, Benjamin and Hoang, Jenny K. and Thayer, David and Tessler, Franklin N. and Middleton, William D. and Mazurowski, Maciej A.},
	year = {2019},
	note = {Publisher: Radiological Society of North America},
	pages = {695--701},
}

@article{li_improved_2018,
	title = {An improved deep learning approach for detection of thyroid papillary cancer in ultrasound images},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-25005-7},
	doi = {10.1038/s41598-018-25005-7},
	abstract = {Unlike daily routine images, ultrasound images are usually monochrome and low-resolution. In ultrasound images, the cancer regions are usually blurred, vague margin and irregular in shape. Moreover, the features of cancer region are very similar to normal or benign tissues. Therefore, training ultrasound images with original Convolutional Neural Network (CNN) directly is not satisfactory. In our study, inspired by state-of-the-art object detection network Faster R-CNN, we develop a detector which is more suitable for thyroid papillary carcinoma detection in ultrasound images. In order to improve the accuracy of the detection, we add a spatial constrained layer to CNN so that the detector can extract the features of surrounding region in which the cancer regions are residing. In addition, by concatenating the shallow and deep layers of the CNN, the detector can detect blurrier or smaller cancer regions. The experiments demonstrate that the potential of this new methodology can reduce the workload for pathologists and increase the objectivity of diagnoses. We find that 93:5\% of papillary thyroid carcinoma regions could be detected automatically while 81:5\% of benign and normal tissue could be excluded without the use of any additional immunohistochemical markers or human intervention.},
	language = {en},
	number = {1},
	urldate = {2022-05-16},
	journal = {Scientific Reports},
	author = {Li, Hailiang and Weng, Jian and Shi, Yujian and Gu, Wanrong and Mao, Yijun and Wang, Yonghua and Liu, Weiwei and Zhang, Jiajie},
	month = apr,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cancer imaging, Computational science},
	pages = {6600},
}

@article{liu_deep_2020,
	title = {Deep {Learning} for {Generic} {Object} {Detection}: {A} {Survey}},
	volume = {128},
	issn = {1573-1405},
	shorttitle = {Deep {Learning} for {Generic} {Object} {Detection}},
	url = {https://doi.org/10.1007/s11263-019-01247-4},
	doi = {10.1007/s11263-019-01247-4},
	abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.},
	language = {en},
	number = {2},
	urldate = {2022-05-16},
	journal = {International Journal of Computer Vision},
	author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik√§inen, Matti},
	month = feb,
	year = {2020},
	keywords = {Convolutional neural networks, Deep learning, Object detection, Object recognition},
	pages = {261--318},
}

@article{sharifi_deep_2021,
	title = {Deep learning on ultrasound images of thyroid nodules},
	volume = {41},
	issn = {0208-5216},
	url = {https://www.sciencedirect.com/science/article/pii/S0208521621000152},
	doi = {10.1016/j.bbe.2021.02.008},
	abstract = {Due to safety, easy accessibility, noninvasively and cost-effectiveness of ultrasound imaging, this technology becomes one of the main contributors for analyzing thyroid nodules. However, interpretation of ultrasound images is a challenging task that subjects to the radiologist‚Äôs prior medical knowledge and observational skills. There is a significant need for reliable, objective, and automated approaches for the meaningful assessment of ultrasound images. Many areas of machine learning including computer vision and image processing have been revolutionized by the recent advances in the field of deep learning. The current study systematically reviews the existing literatures and evaluates technical characteristics of the deep learning applications on the ultrasound images of thyroid nodules. In this review, all of the included studies have been published from 2017 to 2020 indicating the recent growing interest in the utilization of deep learning-based techniques for assessment of ultrasound images of thyroid nodules. Although deep learning has demonstrated potential for analyzing thyroid nodules‚Äô ultrasound images, this review highlights several existing barriers that need to be addressed in future works such as dealing with data limitation, generating public and valid datasets, and determining standard evaluation metrics. This survey outlines several methods (e.g., data augmentation and transfer learning) recently proposed to address similar challenges in other fields. Furthermore, to improve the diagnostic accuracy of the deep learning models, utilization of complementary information with multi-modal images are suggested. The protocol of this systematic review was registered in the PROSPERO database (CRD42020168346).},
	language = {en},
	number = {2},
	urldate = {2022-05-16},
	journal = {Biocybernetics and Biomedical Engineering},
	author = {Sharifi, Yasaman and Bakhshali, Mohamad Amin and Dehghani, Toktam and DanaiAshgzari, Morteza and Sargolzaei, Mahdi and Eslami, Saeid},
	month = apr,
	year = {2021},
	keywords = {Computer-assisted diagnosis, Deep learning, Medical diagnosis, Thyroid nodules, Ultrasonography},
	pages = {636--655},
}

@article{keramidas_nd_2012,
	title = {Œ§{ND}: {A} {Thyroid} {Nodule} {Detection} {System} for {Analysis} of {Ultrasound} {Images} and {Videos}},
	volume = {36},
	issn = {1573-689X},
	shorttitle = {Œ§{ND}},
	url = {https://doi.org/10.1007/s10916-010-9588-7},
	doi = {10.1007/s10916-010-9588-7},
	abstract = {In this paper, we present a computer-aided-diagnosis (CAD) system prototype, named TND (Thyroid Nodule Detector), for the detection of nodular tissue in ultrasound (US) thyroid images and videos acquired during thyroid US examinations. The proposed system incorporates an original methodology that involves a novel algorithm for automatic definition of the boundaries of the thyroid gland, and a novel approach for the extraction of noise resilient image features effectively representing the textural and the echogenic properties of the thyroid tissue. Through extensive experimental evaluation on real thyroid US data, its accuracy in thyroid nodule detection has been estimated to exceed 95\%. These results attest to the feasibility of the clinical application of TND, for the provision of a second more objective opinion to the radiologists by exploiting image evidences.},
	language = {en},
	number = {3},
	urldate = {2022-05-16},
	journal = {Journal of Medical Systems},
	author = {Keramidas, Eystratios G. and Maroulis, Dimitris and Iakovidis, Dimitris K.},
	month = jun,
	year = {2012},
	keywords = {Computer-aided-diagnosis, Nodule, Thyroid, Ultrasound},
	pages = {1271--1281},
}
